---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: specialization_project
    language: python
    name: python3
---

```{python}
# Fixing automatic autoreload
# %load_ext autoreload
# %autoreload 2


# Making sure we are running the code from the root directory
import os 
current_directory = os.getcwd()
if current_directory.endswith("notebooks"):
    os.chdir("..")
    print("Changed directory to:", os.getcwd())
else:
    print("Directory was already correct, so did not change.")
```

```{python}
import os
import torch

from glob import glob
from monai.losses import DiceLoss
from monai.data import ImageDataset
from torch.utils.data import DataLoader
from torch.optim import Adam

from src.deeplabv3.network.modeling import _segm_resnet
from src.train_utils import train
```

```{python}

num_epochs = 2
batch_size = 2
data_dir = "/cluster/projects/vc/data/mic/open/OCELOT/ocelot_data"
checkpoint_interval = 5

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Training with the following parameters:")
print(f"Data directory: {data_dir}")
print(f"Number of epochs: {num_epochs}")
print(f"Batch size: {batch_size}")
print(f"Checkpoint interval: {checkpoint_interval}")
print(f"Device: {device}")
print(f"Number of GPUs: {torch.cuda.device_count()}")

train_seg_files = glob(os.path.join(data_dir, "annotations/train/segmented_cell/*"))
train_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in train_seg_files
]
train_image_files = [
    os.path.join(data_dir, "images/train/cell", image_number + ".jpg")
    for image_number in train_image_numbers
]

val_seg_files = glob(os.path.join(data_dir, "annotations/val/segmented_cell/*"))
val_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in val_seg_files
]
val_image_files = [
    os.path.join(data_dir, "images/val/cell", image_number + ".jpg")
    for image_number in val_image_numbers
]

# Create dataset and dataloader
train_dataset = ImageDataset(
    image_files=train_image_files, seg_files=train_seg_files
)
val_dataset = ImageDataset(image_files=val_image_files, seg_files=val_seg_files)

train_dataloader = DataLoader(
    dataset=train_dataset, batch_size=batch_size, shuffle=True
)
val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size)

model = _segm_resnet(
    name="deeplabv3plus",
    backbone_name="resnet50",
    num_classes=3,
    output_stride=8,
    pretrained_backbone=True,
)
model.to(device)

loss_function = DiceLoss(softmax=True)
optimizer = Adam(model.parameters(), lr=1e-3)

train(
    num_epochs=num_epochs,
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    model=model,
    loss_function=loss_function,
    optimizer=optimizer,
    device=device,
    checkpoint_interval=checkpoint_interval,
    break_after_one_iteration=False,
)
```
