---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: master_project
    language: python
    name: python3
---

```{python}
import os 

# Making sure we are running the code from the root directory
current_directory = os.getcwd()
if current_directory.endswith("notebooks"):
    os.chdir("..")
    print("Changed directory to:", os.getcwd())
else:
    print("Directory was already correct, so did not change.")
```

## Load ViTMAE


python scripts/run_mae.py \
    --dataset_name "/cluster/projects/vc/data/mic/open/OCELOT/ocelot_data" \
    --train_dir "images/train/cell/*" \
    --output_dir "/cluster/home/bendikgh/master_project/outputs/ViTMAE" \
    --remove_unused_columns False \
    --label_names pixel_values \
    --do_train \
    --do_eval \
    --overwrite_output_dir \
    --image_processor_name "/cluster/home/bendikgh/master_project/config/preprocessor_config.json" \
    --config_name "/cluster/home/bendikgh/master_project/config/ViT_config" \
    --num_train_epochs 300


```{python}
from transformers import ViTMAEConfig, ViTMAEModel, AutoImageProcessor
```

```{python}
# Initializing a ViT MAE vit-mae-base style configuration
configuration = ViTMAEConfig(image_size=1024, patch_size=64)

# Initializing a model (with random weights) from the vit-mae-base style configuration
model = ViTMAEModel(configuration)

# Accessing the model configuration
configuration = model.config
```

```{python}
configuration.to_json_file("config/ViT_config")
```

## Make datasets and dataloaders for tissue

```{python}
import argparse
import os
import torch
import albumentations as A

from glob import glob
from monai.losses import DiceLoss
from torch.utils.data import DataLoader
from torch.optim import Adam
from torch.nn import MSELoss

from src.deeplabv3.network.modeling import _segm_resnet
from src.utils.utils_train import train
from src.dataset import TissueDataset
```

```{python}
data_dir = "/cluster/projects/vc/data/mic/open/OCELOT/ocelot_data"
```

```{python}
train_tissue_seg_files = glob(os.path.join(data_dir, "annotations/train/tissue/*"))
train_tissue_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in train_tissue_seg_files
]
train_tissue_image_files = [
    os.path.join(data_dir, "images/train/tissue", image_number + ".jpg")
    for image_number in train_tissue_image_numbers
]

val_tissue_seg_files = glob(os.path.join(data_dir, "annotations/val/tissue/*"))
val_tissue_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in val_tissue_seg_files
]
val_tissue_image_files = [
    os.path.join(data_dir, "images/val/tissue", image_number + ".jpg")
    for image_number in val_tissue_image_numbers
]
```

```{python}
## TODO: Make dataset classes for MAE
```

```{python}
batch_size = 1

train_tissue_dataset = TissueDataset(
    image_files=train_tissue_image_files,
    seg_files=train_tissue_seg_files,
)
val_tissue_dataset = TissueDataset(
    image_files=val_tissue_image_files, seg_files=val_tissue_seg_files
)

train_tissue_dataloader = DataLoader(
    dataset=train_tissue_dataset, batch_size=batch_size, drop_last=True, shuffle=True
)
val_tissue_dataloader = DataLoader(
    dataset=val_tissue_dataset, batch_size=batch_size, drop_last=True
)
```

```{python}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# image_processor = AutoImageProcessor.from_pretrained("facebook/vit-mae-base")
optimizer = Adam(model.parameters(), lr=1e-4)
loss_function = MSELoss()
num_epochs = 100
checkpoint_interval = 10,
```

```{python}
# Github implementation:
# https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining

# Model doc:
# https://huggingface.co/docs/transformers/model_doc/vit_mae#transformers.ViTMAEForPreTraining

model.train()

for images, _ in train_tissue_dataloader:
    
    mask = torch.rand(images.shape[0], 1, images.shape[2], images.shape[3]) > 0.5
    masked_images = images * mask

    # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward pass
    outputs = model(masked_images)

    # Calculate loss
    loss = loss_function(outputs, images)  # Compare with original images
    loss.backward()
    optimizer.step()
```

```{python}

```
