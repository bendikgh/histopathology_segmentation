---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: specialization_project
    language: python
    name: python3
---

```{python}
# Fixing automatic autoreload
# %load_ext autoreload
# %autoreload 2
```

```{python}
import os 

# Making sure we are running the code from the root directory
current_directory = os.getcwd()
if current_directory.endswith("notebooks"):
    os.chdir("..")
    print("Changed directory to:", os.getcwd())
else:
    print("Directory was already correct, so did not change.")
```

```{python}
import torch
import albumentations as A

from glob import glob
from monai.data import DataLoader
from monai.losses import DiceLoss
from torch.optim import Adam

from src.deeplabv3.network.modeling import _segm_resnet
from src.dataset import CellOnlyDataset
from src.train_utils import train

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"device: {device}")
```

```{python}
data_path = "/cluster/projects/vc/data/mic/open/OCELOT/ocelot_data"

train_seg_files = glob(os.path.join(data_path, "annotations/train/segmented_cell/*"))
train_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in train_seg_files
]
train_image_files = [
    os.path.join(data_path, "images/train/cell", image_number + ".jpg")
    for image_number in train_image_numbers
]

val_seg_files = glob(os.path.join(data_path, "annotations/val/segmented_cell/*"))
val_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in val_seg_files
]
val_image_files = [
    os.path.join(data_path, "images/val/cell", image_number + ".jpg")
    for image_number in val_image_numbers
]

test_seg_files = glob(os.path.join(data_path, "annotations/test/segmented_cell/*"))
test_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in test_seg_files
]
test_image_files = [
    os.path.join(data_path, "images/test/cell", image_number + ".jpg")
    for image_number in test_image_numbers
]
```

```{python}
transforms = A.Compose([
    A.GaussianBlur(blur_limit=(3, 7), p=0.5),  # You can adjust the blur limit
    A.GaussNoise(var_limit=(0.1, 0.3), p=0.5),  # Adjust var_limit for noise intensity
    A.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.2, hue=0.1, p=1),
    A.HorizontalFlip(p=0.5),
    A.RandomRotate90(p=0.5)
])
```

```{python}
train_dataset = CellOnlyDataset(image_files=train_image_files, seg_files=train_seg_files, transform=transforms)
val_dataset = CellOnlyDataset(image_files=val_image_files, seg_files=val_seg_files)
test_dataset = CellOnlyDataset(image_files=test_image_files, seg_files=test_seg_files)

train_dataloader = DataLoader(dataset=train_dataset, batch_size=2, drop_last=True)
val_dataloader = DataLoader(dataset=val_dataset)
test_dataloader = DataLoader(dataset=test_dataset)
```

```{python}
backbone = "resnet34"
dropout_rate = 0.3
model = _segm_resnet(
    name="deeplabv3plus",
    backbone_name=backbone,
    num_classes=3,
    output_stride=8,
    pretrained_backbone=True,
    dropout_rate=dropout_rate
)
model.to(device)
print()
```

```{python}
loss_function = DiceLoss(softmax=True)
optimizer = Adam(model.parameters(), lr=1e-3)
```

```{python}
num_epochs = 2
checkpoint_interval = 3

training_losses, validation_losses = train(
    num_epochs=num_epochs,
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    model=model,
    loss_function=loss_function,
    optimizer=optimizer,
    device=device,
    checkpoint_interval=checkpoint_interval,
    break_after_one_iteration=False, 
    dropout_rate=dropout_rate, 
    backbone=backbone
)
print(training_losses)
print(validation_losses)
```
