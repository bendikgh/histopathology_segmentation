---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: master_project
    language: python
    name: python3
---

```{python}
# Fixing automatic autoreload
# %load_ext autoreload
# %autoreload 2
```

```{python}
import os 

# Making sure we are running the code from the root directory
current_directory = os.getcwd()
if current_directory.endswith("notebooks"):
    os.chdir("..")
    print("Changed directory to:", os.getcwd())
else:
    print("Directory was already correct, so did not change.")
```

```{python}
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import albumentations as A

from datetime import datetime
from monai.losses import DiceLoss
from glob import glob
from torch.utils.data import DataLoader
from torch.optim import AdamW
from skimage.feature import peak_local_max
from torch.nn.functional import softmax, interpolate
from transformers import (
    SegformerForSemanticSegmentation,
    SegformerConfig,
    SegformerImageProcessor,
    get_polynomial_decay_schedule_with_warmup,
)

from src.utils.utils_train import (
    run_training_segformer,
    run_validation_segformer,
    train,
)
from src.dataset import SegformerDataset

sns.set_theme()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")
```

## Data preprocessing

```{python}
data_dir = "/cluster/projects/vc/data/mic/open/OCELOT/ocelot_data"
train_seg_files = glob(os.path.join(data_dir, "annotations/train/segmented_cell/*"))
train_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in train_seg_files
]
train_image_files = [
    os.path.join(data_dir, "images/train/cell", image_number + ".jpg")
    for image_number in train_image_numbers
]

val_seg_files = glob(os.path.join(data_dir, "annotations/val/segmented_cell/*"))
val_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in val_seg_files
]
val_image_files = [
    os.path.join(data_dir, "images/val/cell", image_number + ".jpg")
    for image_number in val_image_numbers
]

test_seg_files = glob(os.path.join(data_dir, "annotations/test/segmented_cell/*"))
test_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in test_seg_files
]
test_image_files = [
    os.path.join(data_dir, "images/test/cell", image_number + ".jpg")
    for image_number in test_image_numbers
]
```

```{python}
transforms = A.Compose(
    [
        A.GaussianBlur(blur_limit=(3, 7), p=0.5),
        A.GaussNoise(var_limit=(0.1, 0.3), p=0.5),
        A.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.2, hue=0.1, p=1),
        A.HorizontalFlip(p=0.5),
        A.RandomRotate90(p=0.5),
    ]
)
image_processor = SegformerImageProcessor(do_resize=False, do_rescale=False)
```

```{python}
train_dataset = SegformerDataset(
    train_image_files,
    train_seg_files,
    transform=transforms,
    preprocessor=image_processor.preprocess,
)
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)

val_dataset = SegformerDataset(
    val_image_files,
    val_seg_files,
    preprocessor=image_processor.preprocess,
)
val_loader = DataLoader(val_dataset, batch_size=2)

test_dataset = SegformerDataset(
    test_image_files,
    test_seg_files,
    preprocessor=image_processor.preprocess,
)
test_loader = DataLoader(test_dataset, batch_size=2)
```

```{python}
it = iter(train_loader)
image_batch, label_batch = next(it)
image_batch, label_batch = image_batch.squeeze(), label_batch.squeeze()

print(image_batch.shape)
print(image_batch.unique())
```

```{python}
configuration = SegformerConfig(
  num_labels=3,
  num_channels=3,
)
model = SegformerForSemanticSegmentation(
  configuration
)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Total Parameters: {total_params}")
print(f"Trainable Parameters: {trainable_params}")
```

```{python}
num_epochs = 5
optimizer = AdamW(model.parameters(), lr=5e-5)
scheduler = get_polynomial_decay_schedule_with_warmup(
    optimizer, num_warmup_steps=2, num_training_steps=num_epochs, power=1
)
loss_fn = DiceLoss(softmax=True, to_onehot_y=True)
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
save_name = f"segformer_tryout_{current_time}"

train(
    num_epochs=num_epochs,
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    model=model,
    loss_function=loss_fn,
    optimizer=optimizer,
    device=device,
    save_name=save_name,
    checkpoint_interval=5,
    break_after_one_iteration=True,
    scheduler=scheduler,
    training_func=run_training_segformer,
    validation_function=run_validation_segformer,
)
```

```{python}
image_batch, label_batch = next(iter(val_loader))
print(image_batch.shape)
```

```{python}
outputs = model(pixel_values=image_batch.to(device))
probabilities = softmax(outputs.logits, dim=1)
```

```{python}
print(image_batch.shape)
print(outputs.logits.shape)

argmaxed = probabilities[0].argmax(dim=0).detach().cpu()
print(argmaxed.shape)
print(argmaxed.unique())
```

```{python}
plt.imshow(argmaxed.numpy())
plt.show()
plt.imshow(val_dataset.get_image(0).permute(1, 2, 0).to(torch.uint8))
plt.show()
```

```{python}
def calculate_f1_score_segformer(
    model, dataloader, device, micron_radius: float = 3.0, mpp: float = 0.2
):
    """These are the steps for calculating the F1 score, as given in the paper:

    - True positive: If a detected cell is within a valid distance (3 microns) of a
      target cell, then it is considered a TP
    - False positive: If a detected cell does not fulfill the requirements for a
      TP, then it is considered a FP
    - False Negative: If an annotated cell is not detected, then it is counted as a
      False Negative

    """
    model.eval()
    f1_scores = []
    pixel_radius = micron_radius / mpp
    dataset = dataloader.dataset
    batch_size = dataloader.batch_size

    for batch_idx, (image_batch, mask_batch) in enumerate(dataloader):
        image_batch = image_batch.to(device)
        with torch.no_grad():
            output_batch = model(image_batch).logits

        for idx in range(output_batch.shape[0]):
            output = output_batch[idx]
            output = interpolate(
                output.unsqueeze(0),
                size=mask_batch.shape[-2:],
                mode="bilinear",
                align_corners=False,
            )
            output = output.squeeze(
                0
            )  # Remove the extra batch dimension that interpolate adds

            image_no = batch_idx * batch_size + idx
            cell_annotation_list = dataset.get_cell_annotation_list(image_no)

            # Preparing output for peak_local_max
            softmaxed = torch.softmax(output, dim=0)
            cells, argmaxed = torch.max(softmaxed, axis=0)
            argmaxed = argmaxed.cpu().numpy()
            cells = cells.cpu().numpy()
            peak_points_pred = peak_local_max(
                cells,
                min_distance=20,
                labels=np.logical_or(argmaxed == 1, argmaxed == 2),
                threshold_abs=0.01,
            )

            # # For plotting the predictions and the ground truth on top
            # output = make_prediction_map(output_batch)[0]*255
            # # display results
            # fig, ax = plt.subplots(1, 1, figsize=(8, 3), sharex=True, sharey=True)
            # ax.imshow(output.permute(1, 2, 0))
            # ax.autoscale(False)
            # ax.plot(cell_annotation_list[:, 0], cell_annotation_list[:, 1], 'o', color='black', markersize=0.5)
            # ax.axis('off')
            # ax.set_title('Peak local max')
            # fig.tight_layout()
            # plt.show()

            TP = 0
            FP = 0
            for y, x in peak_points_pred:
                # We check a circle around the point to see if there is a cell in the mask
                # If there is, we count it as a TP
                cell_type = argmaxed[y, x]
                TP_old = TP  # To check if TP changes

                min_distance_squared = (pixel_radius + 1) ** 2
                min_distance_cell = -1

                # Calculate distance vector to cell_annotation_list
                if cell_annotation_list.shape[0] > 0:
                    distance_squared = (x - cell_annotation_list[:, 0]) ** 2 + (
                        y - cell_annotation_list[:, 1]
                    ) ** 2
                    min_distance_squared, min_distance_cell = np.min(
                        distance_squared
                    ), np.argmin(distance_squared)

                if min_distance_squared < pixel_radius**2:
                    if cell_annotation_list[min_distance_cell][2] != cell_type:
                        TP += 1
                    cell_annotation_list = np.delete(
                        cell_annotation_list, min_distance_cell, axis=0
                    )

                # If we did not find an annotated cell matching this one, then
                # we count it as a FP
                if TP_old == TP:
                    FP += 1

            FN = len(cell_annotation_list)
            f1_score = (2.0 * TP) / (2 * TP + FP + FN)
            f1_scores.append(f1_score)

    return torch.mean(torch.tensor(f1_scores))
```

```{python}
configuration = SegformerConfig(
  num_labels=3,
  num_channels=3,
)
model = SegformerForSemanticSegmentation(
  configuration
)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model.load_state_dict(torch.load("outputs/models/segformer_tryout_20240211_133248_epochs-5.pth"))
model.to(device)
print()
```

```{python}
val_f1_score = calculate_f1_score_segformer(model, val_loader, device)
print(f"val f1 score: {val_f1_score}")
test_f1_score = calculate_f1_score_segformer(model, test_loader, device)
print(f"test f1 score: {test_f1_score}")
```
