---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: specialization_project
    language: python
    name: python3
---

```{python}
# Fixing automatic autoreload
# %load_ext autoreload
# %autoreload 2
```

```{python}
import os 

# Making sure we are running the code from the root directory
current_directory = os.getcwd()
if current_directory.endswith("notebooks"):
    os.chdir("..")
    print("Changed directory to:", os.getcwd())
else:
    print("Directory was already correct, so did not change.")
```

```{python}
import torch
import matplotlib.pyplot as plt 
import seaborn as sns

from glob import glob
from torch.utils.data import DataLoader

from src.deeplabv3.network.modeling import _segm_resnet
from src.dataset import TissueLeakingDataset

sns.set_theme()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")
```

```{python}
data_path = "/cluster/projects/vc/data/mic/open/OCELOT/ocelot_data"
```

```{python}
train_cell_seg = sorted(glob(os.path.join(data_path, "annotations/train/segmented_cell/*")))
train_tissue_seg = []
train_input_img = []

for img_path in train_cell_seg: 
    ending = img_path.split("/")[-1].split(".")[0]
    tissue_seg_path = glob(os.path.join(data_path, "annotations/train/cropped_tissue/" + ending + "*"))[0]
    input_img_path = glob(os.path.join(data_path, "images/train/cell/" + ending + "*"))[0]
    train_tissue_seg.append(tissue_seg_path)
    train_input_img.append(input_img_path)


val_cell_seg = sorted(glob(os.path.join(data_path, "annotations/val/segmented_cell/*")))
val_tissue_seg = []
val_input_img = []

for img_path in val_cell_seg: 
    ending = img_path.split("/")[-1].split(".")[0]
    tissue_seg_path = glob(os.path.join(data_path, "annotations/val/cropped_tissue/" + ending + "*"))[0]
    input_img_path = glob(os.path.join(data_path, "images/val/cell/" + ending + "*"))[0]
    val_tissue_seg.append(tissue_seg_path)
    val_input_img.append(input_img_path)


test_cell_seg = sorted(glob(os.path.join(data_path, "annotations/test/segmented_cell/*")))
test_tissue_seg = []
test_input_img = []

for img_path in test_cell_seg: 
    ending = img_path.split("/")[-1].split(".")[0]
    tissue_seg_path = glob(os.path.join(data_path, "annotations/test/cropped_tissue/" + ending + "*"))[0]
    input_img_path = glob(os.path.join(data_path, "images/test/cell/" + ending + "*"))[0]
    test_tissue_seg.append(tissue_seg_path)
    test_input_img.append(input_img_path)
```

```{python}
train_dataset = TissueLeakingDataset(input_files=train_input_img, cell_seg_files=train_cell_seg, tissue_seg_files=train_tissue_seg)
val_dataset = TissueLeakingDataset(input_files=val_input_img, cell_seg_files=val_cell_seg, tissue_seg_files=val_tissue_seg)
test_dataset = TissueLeakingDataset(input_files=test_input_img, cell_seg_files=test_cell_seg, tissue_seg_files=test_tissue_seg)

train_dataloader = DataLoader(dataset=train_dataset, batch_size=2, drop_last=True)
val_dataloader = DataLoader(dataset=val_dataset, batch_size=2, drop_last=True)
test_dataloader = DataLoader(dataset=test_dataset, batch_size=2, drop_last=True)
```

```{python}
from monai.metrics import DiceMetric
from monai.transforms import Compose, AsDiscrete

def calculate_dice_score(dataloader, model, device): 

    dice_metric = DiceMetric(include_background=True, reduction="mean")
    post_pred = Compose([AsDiscrete(argmax=True, dim=1, to_onehot=3)])

    model.eval()
    for (images, masks) in dataloader: 
        images, masks = images.to(device), masks.to(device)
        with torch.no_grad(): 
            outputs = model(images)
        outputs = post_pred(outputs)
        dice_metric(outputs, masks)

    dice_score = dice_metric.aggregate().item()
    dice_metric.reset()

    return dice_score
```

```{python}
backbone = "resnet34"
dropout_rate = 0.3
model = _segm_resnet(
    name="deeplabv3plus",
    backbone_name=backbone,
    num_classes=3,
    output_stride=8,
    pretrained_backbone=True,
    dropout_rate=dropout_rate,
    num_channels=6
)
model.to(device)
model.load_state_dict(torch.load("outputs/models/2023-12-04_18-38-55_deeplabv3plus_tissue_leaking_lr-0.0001_dropout-0.3_backbone-resnet34_epochs-100.pth"))
model.eval()

test_dice_score = calculate_dice_score(test_dataloader, model, device)
val_dice_score = calculate_dice_score(val_dataloader, model, device)
print("Tissue Leaking model, lr=0.0001, dropout=0.3, backbone=resnet34")
print(f"Dice score on test set: {test_dice_score}")
print(f"Dice score on validation set: {val_dice_score}")
```

```{python}
import torch.nn.functional as F
from skimage.feature import peak_local_max

from src.utils import create_cell_segmentation_image

img, seg = next(iter(test_dataloader))
img, seg = img.to(device), seg.to(device)

model.eval()
with torch.no_grad():
    output = model(img)
probabilities = F.softmax(output, dim=1)
print(probabilities.size())
```

```{python}
argmaxed = torch.argmax(output, dim=1)
plt.imshow(argmaxed[0].detach().cpu())
plt.show()
print(argmaxed.size())
```

```{python}
cells = argmaxed[0].cpu().numpy()
background_cells = peak_local_max(cells, min_distance=20, labels=cells == 1)
tumor_cells = peak_local_max(cells, min_distance=20, labels=cells == 2)

all_cells = peak_local_max(cells, min_distance=20, exclude_border=True)

background_cells_with_labels = [(x, y, 2) for x, y in background_cells]
tumor_cells_with_labels = [(x, y, 1) for x, y in tumor_cells]

coords_with_labels = background_cells_with_labels + tumor_cells_with_labels
```

```{python}
segmented_image = create_cell_segmentation_image(
    annotated_data=coords_with_labels, 
    cell_mpp=0.2
)
```

```{python}
print(segmented_image.shape)
plt.imshow(segmented_image*255)
plt.show()
plt.imshow(seg.cpu()[0].permute(1, 2, 0))
plt.show()
```

```{python}
input_image = img[0, :3].cpu()
plt.figure(figsize=(12, 4))
#plt.scatter(background_cells[:, 1], background_cells[:, 0], s=7, facecolors="none", edgecolors="y")
#plt.scatter(tumor_cells[:, 1], tumor_cells[:, 0], s=7, facecolors="none", edgecolors="g")
plt.subplot(1, 3, 1)
plt.scatter(all_cells[:, 1], all_cells[:, 0], s=7, facecolors="none", edgecolors="y")
plt.xlim(0, 1024)
plt.ylim(0, 1024)
plt.gca().invert_yaxis()
plt.imshow(argmaxed[0].detach().cpu())
plt.title("Argmaxed model output\n + peak_local_max")
plt.axis("off")

plt.subplot(1, 3, 2)
plt.imshow(seg.cpu()[0].permute(1, 2, 0))
#plt.imshow(argmaxed[0].detach().cpu())
plt.scatter(all_cells[:, 1], all_cells[:, 0], s=7, facecolors="none", edgecolors="black")
plt.title("Target segmentation mask \n+ peak_local_max of outputs")
plt.axis("off")

plt.subplot(1, 3, 3)
plt.imshow(input_image.permute(1, 2, 0))
plt.title("Input image")
plt.axis("off")
plt.tight_layout()
plt.show()
```

```{python}
training_losses = [0.6816265473560411, 0.6373964085870859, 0.6050575545855931, 0.5816195339572673, 0.5736042905826958, 0.5665267036885632, 0.556762449595393, 0.5569390776206036, 0.5490029028483799, 0.5448756436912381, 0.5434097805801703, 0.5366764117260369, 0.535835996574285, 0.5354424161570412, 0.5326408883746789, 0.5310142216633778, 0.5320193676316008, 0.5279067316833808, 0.5224982962316397, 0.5232757104902851, 0.518204577723328, 0.5208658788885389, 0.5165681249024917, 0.5166864662754292, 0.5163541326717455, 0.5154696885420351, 0.5141025124763956, 0.5109306884055235, 0.51251192178045, 0.5115902369119683, 0.5112933346203395, 0.5093840567433104, 0.5074319200856345, 0.5081957809779108, 0.5062605373713435, 0.5062366578043723, 0.5050369044955896, 0.5040192816938672, 0.5051999974007509, 0.5019889577310912, 0.5016482156150195, 0.5013132977242373, 0.49953997135162354, 0.49983558910233633, 0.5008069623489769, 0.5003583558968135, 0.4983057410133128, 0.4987408062633203, 0.4958089571826312, 0.49764298784489536, 0.495467340459629, 0.49739971817756184, 0.49476122247929477, 0.4947957822254726, 0.49297429347524835, 0.4930061004599746, 0.4926298254606675, 0.4933912954768356, 0.4912620691620574, 0.4890154965069829, 0.48934625241221213, 0.4888954351142961, 0.490931263383554, 0.4907783580069639, 0.48850226402282715, 0.48635075591048416, 0.48472729507757695, 0.4865133111574212, 0.48329997184325235, 0.48535742686719313, 0.48538472822734285, 0.48226866916734346, 0.4817915035753834, 0.4798404464916307, 0.48328542101139926, 0.4835352459732367, 0.4812447319225389, 0.48129828122197366, 0.4818223708746385, 0.4803693349264106, 0.47915463423242377, 0.4776195956736195, 0.4798867878865223, 0.4796409467045142, 0.48153935768166367, 0.4763504382298917, 0.47814940068186546, 0.47721805073777024, 0.4765826846872057, 0.47642143587676844, 0.4752552466733115, 0.4741565700696439, 0.4738978488104684, 0.47239517496556654, 0.4722962306470287, 0.47409167581675005, 0.4697180274797946, 0.4732159169352784, 0.46990032585299746, 0.4706717595762136, 0.47215968735364017, 0.46930171883836086, 0.4694587302451231, 0.4702237734989244, 0.4709209726781261, 0.469053256268404, 0.4657025434532944, 0.46675994870614035, 0.46571988840492407, 0.46536535328748274, 0.4641509852847274, 0.46811356653972547, 0.4667716932540037, 0.46282497291662256, 0.4653707602802588, 0.4663413610993599, 0.4625845606229743, 0.4621932245030695, 0.4590114938969515, 0.46311064642302846, 0.46176393299686663, 0.4587804973125458, 0.45957679103831856, 0.4589533447002878, 0.45540642555879085, 0.46063609816590134, 0.45569796890628583, 0.45674858531173396, 0.45615617596373265, 0.45706414446538807, 0.4576328110938169, 0.45484874199847786, 0.45416522451809477, 0.45643073806957324, 0.45427845144758416, 0.45381955224640513, 0.45250301215113425, 0.4518446569540063, 0.45303280134590307, 0.45226852261290257, 0.44896712473460604, 0.4499532525636712, 0.4515604261232882, 0.44974094568466655, 0.45339642677988323, 0.45138430169650484, 0.45045063142873804, 0.4478226741965936, 0.4477335634280224, 0.44549401986355686, 0.4519914467724002, 0.4483773361663429, 0.4486247684274401, 0.44449910825612593, 0.44221534047807964, 0.4444568887048838, 0.4432898096892299, 0.44343284867247756, 0.4447852811034845, 0.44882030937136436, 0.444249511981497, 0.442794926312505, 0.4417589367652426, 0.4409597880986272, 0.44150338914929604, 0.4381643059302349, 0.4397137025181128, 0.43827422845120334, 0.4388477960411383, 0.4389923956929421, 0.44056247691718897, 0.43656487793338544, 0.4395182211788333, 0.4369427586088375, 0.43533915281295776, 0.43246729519902444, 0.4387429672844556, 0.4366171937815997, 0.4366897685187204, 0.43271244362908967, 0.43334441768879794, 0.4351888274659916, 0.436783462154622, 0.4353988590289135, 0.43636184565874997, 0.432642898997482, 0.4310987762042454, 0.43079706965660564, 0.43277077833000493, 0.43350028930878154, 0.42906200885772705, 0.42694936236556696, 0.4283568591487651, 0.42688814961180394, 0.4299958032004687, 0.42542219466092634, 0.4268152069072334, 0.43163986352025246, 0.43008306196757723, 0.4317478519313189, 0.43227467670732617, 0.4319496957623229, 0.43106420794311834, 0.4258911670470724, 0.4235396780529801, 0.4271812809973347, 0.42287457232572595, 0.4259882344275105, 0.42508088934178256, 0.4243379673179315, 0.42326291909023206, 0.426359700913332, 0.42306262741283496, 0.426185561686146, 0.4244688457372237, 0.4233275706670722, 0.4248920527039742, 0.42639175239874394, 0.4201137776277503, 0.4226615374185601, 0.42103806259680765, 0.4240529671007273, 0.4247783647508037, 0.42393405644261106, 0.42099993387047124, 0.4194071809856259, 0.4171909525686381, 0.41926929354667664, 0.41866885216868654, 0.41575322771558954, 0.42207616081043164, 0.4210524115027214, 0.41559189558029175, 0.4164760082351918, 0.4154514773767822, 0.4190176311804324, 0.41742187373492184, 0.41818568110466003, 0.41555897617826654, 0.4117157380191647, 0.41698106393522144, 0.4154576744352068, 0.40980967940116414, 0.4139198083050397, 0.4099227037965035, 0.4132142821136786, 0.4140240343249574, 0.4141340109766746, 0.41353602312049087, 0.4101058731273729, 0.4036317388622128, 0.3794860365439434, 0.360679636804425, 0.3575720580256715, 0.3539987556180175, 0.353141113203399, 0.3481139498097556, 0.3416356626822024, 0.35322418200726413, 0.3412562769894697, 0.34158517268239236, 0.3392329118689712, 0.3396653289697608, 0.34237997020993916, 0.3391019677629276, 0.33541919138966775, 0.3353153917254234, 0.34053593448230196, 0.3441357132123441, 0.3366021343639919, 0.33992040339781315, 0.33622488561941655, 0.33833370099262317, 0.34569465566654595, 0.3368644367675392, 0.3343900381302347, 0.33401536880707255, 0.3394279735428946, 0.337051586228974, 0.334201377873518, 0.3483441812651498, 0.3463935091787455, 0.334369169814246, 0.335361115786494, 0.33430301534886264, 0.3347654838343056, 0.33705342424159146, 0.3361697631831072, 0.33509441267470924, 0.33654010660794315, 0.3449721774276422, 0.33877915022324545, 0.3414808335352917, 0.3381431148368485, 0.3358419811239048, 0.338483562274855, 0.33282941883924055, 0.34010555184617336, 0.33763027951425434, 0.3325781651905605]
val_losses = [0.6484823787913603, 0.6050602302831762, 0.5639426532913657, 0.5452724379651687, 0.5503328547758215, 0.530257044469609, 0.5352154058568618, 0.5204567926771501, 0.5164669874836417, 0.5137251808362848, 0.517364510718514, 0.6475581421571619, 0.5049816930995268, 0.5019432341351229, 0.5037699236589319, 0.5086038708686829, 0.5067627096877378, 0.5833679735660553, 0.5274711391505074, 0.5070031408001395, 0.49204832490752726, 0.4983741343021393, 0.4896012562162736, 0.4929051118738511, 0.4907990623922909, 0.4854371056837194, 0.4862033426761627, 0.4918150305747986, 0.48280319396187277, 0.484990211094127, 0.48689886051065784, 0.48559052803937125, 0.48792818539282856, 0.47962072316338034, 0.48162464359227347, 0.4816305514644174, 0.488113613689647, 0.48379246802891, 0.48195871009546165, 0.4806018121102277, 0.4851138907320359, 0.4852576080490561, 0.4820343010565814, 0.4806674245525809, 0.4783886583412395, 0.4759443787967457, 0.4791682976133683, 0.47684736988123727, 0.4815613101510441, 0.47453703599817615, 0.4752241390592912, 0.4767185414538664, 0.4773283337845522, 0.4769660129266627, 0.47659602936576395, 0.47628714757807117, 0.4742969432297875, 0.47152768163120046, 0.47761623123112845, 0.4802348122877233, 0.4748622757547042, 0.4770737185197718, 0.4752291458494523, 0.47817224439452677, 0.47616784186924205, 0.4727504744249232, 0.48139634027200584, 0.48185208264519186, 0.4730805824784672, 0.47367044406778674, 0.470687235102934, 0.472293206874062, 0.4754050794769736, 0.4697782905662761, 0.4754915640634649, 0.47326139667454886, 0.47616180076318626, 0.47429492719033184, 0.4725095594630522, 0.47152984317611246, 0.46821267464581656, 0.47011326516375823, 0.4689611862687504, 0.47910509389989514, 0.4683277940048891, 0.46931130570523877, 0.4708247903515311, 0.46981820288826437, 0.4706785976886749, 0.4677908876362969, 0.47176985530292287, 0.46842164502424355, 0.4711442256675047, 0.4696991426103255, 0.47018585134955015, 0.4700814468019149, 0.46840864069321575, 0.46738115654272194, 0.46921570336117463, 0.4688489892903496, 0.46916431363891153, 0.46826952345231, 0.4677610590177424, 0.4693276601679185, 0.4652647113098818, 0.46488526989431944, 0.4661767850903904, 0.47018610554582935, 0.4634701083688175, 0.46693674606435437, 0.4741898967939265, 0.47138338930466595, 0.46426504675079794, 0.4756353444912854, 0.4662785495028776, 0.4687907537993263, 0.4669950253823224, 0.46616532171473785, 0.4745493320857777, 0.4676029822405647, 0.4673889892942765, 0.4738258666851941, 0.46069977563970227, 0.4687066726824817, 0.47228802126996655, 0.47672385503264036, 0.4705705818007974, 0.4675343930721283, 0.4689972383134505, 0.46905264083076925, 0.4658391826293048, 0.46766737804693337, 0.47100234031677246, 0.47944536454537334, 0.4698023971389322, 0.4685494023210862, 0.466271311044693, 0.47177708674879637, 0.4700353478684145, 0.47075910077375527, 0.479439856374965, 0.46580098832354827, 0.4686811320921954, 0.4639737045063692, 0.4656171903890722, 0.47228646979612465, 0.46921944793532877, 0.47003764089416056, 0.4680285138242385, 0.46935179829597473, 0.46647748877020445, 0.47732383012771606, 0.46624627183465395, 0.4670558063422932, 0.4650103660190807, 0.4694131700431599, 0.4660726242205676, 0.47352810116375194, 0.46481429303393645, 0.4715811329729417, 0.4674766554551966, 0.4785720670924467, 0.46843811869621277, 0.480000343392877, 0.4706380104317385, 0.46630457394263325, 0.46990128299769235, 0.4628215730190277, 0.4673069329822765, 0.46820374622064476, 0.4712279701934141, 0.4803961164811078, 0.4634109262157889, 0.4683438644689672, 0.4664347469806671, 0.4664166359340443, 0.46421732972649965, 0.4699685380739324, 0.46789107953800874, 0.4648853540420532, 0.46725787485347076, 0.4719669450731838, 0.4683137641233556, 0.4695506464032566, 0.4669937894624822, 0.4656020560685326, 0.47178298059631796, 0.4774126322830425, 0.4620717273038976, 0.47180649638175964, 0.47138795607230244, 0.4713090851026423, 0.4695242906317991, 0.46638209328931923, 0.4691949416609371, 0.4720211940653184, 0.4663550941383137, 0.4768238102688509, 0.4643632085884319, 0.46807752812609954, 0.47802798362339244, 0.48098553278866935, 0.47441007810480457, 0.469961578355116, 0.46681176038349376, 0.4748846660642063, 0.4768632264698253, 0.46390261369593006, 0.46772507534307595, 0.47180850365582633, 0.473681216730791, 0.472969007842681, 0.46845009747673483, 0.47093403164078207, 0.48076338277143593, 0.4661901488023646, 0.46927494511884804, 0.4665280738297631, 0.4719778316862443, 0.47309300478766947, 0.46486351770513196, 0.46636440298136544, 0.4709041486768162, 0.46015127792077903, 0.46397262986968546, 0.4674511113587548, 0.4636106350842644, 0.4688198198290432, 0.46377018094062805, 0.47397204006419463, 0.4805029858561123, 0.46369895514319925, 0.47178887794999513, 0.47025973831906037, 0.46328551103087035, 0.47462352935005636, 0.4700235005687265, 0.4608183555743274, 0.46960923952214856, 0.46642067327218895, 0.4632412791252136, 0.48059483135447784, 0.4702894214321585, 0.4671071452253005, 0.4769115412936491, 0.4662361618350534, 0.4724045150420245, 0.48061348760829253, 0.4753103098448585, 0.4450343584313112, 0.4385901233729194, 0.38177820514230165, 0.38887443261988025, 0.38243576358346376, 0.38354357025202584, 0.3831683642723981, 0.37960077208631177, 0.4007214325315812, 0.3799986401024987, 0.3867606766083661, 0.3744363644543816, 0.38165026903152466, 0.3770580256686491, 0.38869450022192564, 0.38612231787513285, 0.38560090345494885, 0.37984875545782204, 0.38388824287582846, 0.3784891209181617, 0.3819423054947573, 0.380402568508597, 0.37837516209658456, 0.3814518311444451, 0.3827669480267693, 0.3789021267610438, 0.38561293307472677, 0.38969159652205076, 0.3839784527526182, 0.3852165481623481, 0.37786390500910144, 0.3822941254166996, 0.3775956245029674, 0.37888576703913074, 0.37813490979811726, 0.37740429534631614, 0.3684225117459017, 0.3769322335720062, 0.37291354291579304, 0.3753246331916136, 0.3755055192638846, 0.3779146355741164, 0.3730652542675243, 0.38084742426872253, 0.37511930044959574, 0.38105743772843303, 0.37207814875771017, 0.3752047103994033, 0.38807527282658744, 0.373065545278437, 0.38757161883746877]
plt.figure(figsize=(20, 6))
plt.plot(training_losses, label="Training Loss")
plt.plot(val_losses, label="Validation Loss")
plt.title("Training and Validation Loss for experiment 2")
plt.xlabel("Number of epochs")
plt.ylabel("Dice Loss")
plt.legend()
plt.show()
```
