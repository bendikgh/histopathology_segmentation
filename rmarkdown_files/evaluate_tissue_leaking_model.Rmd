---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: specialization_project
    language: python
    name: python3
---

```{python}
# Fixing automatic autoreload
# %load_ext autoreload
# %autoreload 2
```

```{python}
import os 

# Making sure we are running the code from the root directory
current_directory = os.getcwd()
if current_directory.endswith("notebooks"):
    os.chdir("..")
    print("Changed directory to:", os.getcwd())
else:
    print("Directory was already correct, so did not change.")
```

```{python}
import torch
import matplotlib.pyplot as plt 
import seaborn as sns

from glob import glob
from torch.utils.data import DataLoader
from torch.nn.functional import softmax

import numpy as np

from src.deeplabv3.network.modeling import _segm_resnet
from src.dataset import TissueLeakingDataset

sns.set_theme()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")
```

```{python}
data_path = "/cluster/projects/vc/data/mic/open/OCELOT/ocelot_data"
```

```{python}
train_cell_seg = sorted(glob(os.path.join(data_path, "annotations/train/segmented_cell/*")))
train_tissue_seg = []
train_input_img = []

for img_path in train_cell_seg: 
    ending = img_path.split("/")[-1].split(".")[0]
    tissue_seg_path = glob(os.path.join(data_path, "annotations/train/cropped_tissue/" + ending + "*"))[0]
    input_img_path = glob(os.path.join(data_path, "images/train/cell/" + ending + "*"))[0]
    train_tissue_seg.append(tissue_seg_path)
    train_input_img.append(input_img_path)


val_cell_seg = sorted(glob(os.path.join(data_path, "annotations/val/segmented_cell/*")))
val_tissue_seg = []
val_input_img = []

for img_path in val_cell_seg: 
    ending = img_path.split("/")[-1].split(".")[0]
    tissue_seg_path = glob(os.path.join(data_path, "annotations/val/cropped_tissue/" + ending + "*"))[0]
    input_img_path = glob(os.path.join(data_path, "images/val/cell/" + ending + "*"))[0]
    val_tissue_seg.append(tissue_seg_path)
    val_input_img.append(input_img_path)


test_cell_seg = sorted(glob(os.path.join(data_path, "annotations/test/segmented_cell/*")))
test_tissue_seg = []
test_input_img = []

for img_path in test_cell_seg: 
    ending = img_path.split("/")[-1].split(".")[0]
    tissue_seg_path = glob(os.path.join(data_path, "annotations/test/cropped_tissue/" + ending + "*"))[0]
    input_img_path = glob(os.path.join(data_path, "images/test/cell/" + ending + "*"))[0]
    test_tissue_seg.append(tissue_seg_path)
    test_input_img.append(input_img_path)
```

```{python}
train_dataset = TissueLeakingDataset(input_files=train_input_img, cell_seg_files=train_cell_seg, tissue_seg_files=train_tissue_seg)
val_dataset = TissueLeakingDataset(input_files=val_input_img, cell_seg_files=val_cell_seg, tissue_seg_files=val_tissue_seg)
test_dataset = TissueLeakingDataset(input_files=test_input_img, cell_seg_files=test_cell_seg, tissue_seg_files=test_tissue_seg)

train_dataloader = DataLoader(dataset=train_dataset, batch_size=2, drop_last=True)
val_dataloader = DataLoader(dataset=val_dataset, batch_size=2, drop_last=True)
test_dataloader = DataLoader(dataset=test_dataset, batch_size=2, drop_last=True)
```

```{python}
from monai.metrics import DiceMetric
from monai.transforms import Compose, AsDiscrete

import torch.nn.functional as F
from skimage.feature import peak_local_max

from src.utils.utils import create_cell_segmentation_image

def make_prediction_map(outputs):
    #post_pred = Compose([AsDiscrete(argmax=True, dim=0)])
    maps = []
    convert = np.array([0, 2, 1])
    for output in outputs:
        #argmaxed = post_pred(output)

        # plt.plot()
        # plt.imshow(argmaxed.cpu()*255)
        # plt.show()
        

        # Run peak_local_max individually
        cells, argmaxed = torch.max(softmax(output.cpu(), 0), axis=0)
        mask = argmaxed.cpu().numpy().squeeze()
        
        cells = cells.numpy()

        all_cells = peak_local_max(cells, min_distance=20, labels=np.logical_or(mask==1, mask==2)) #np.logical_or(mask==1, mask==2)

        all_cells_with_values = np.empty((all_cells.shape[0], 3), dtype=all_cells.dtype)
        
        # Flip because of create_cell_segmentation_image and cv2
        all_cells_with_values[:, 0] = all_cells[:, 1]
        all_cells_with_values[:, 1] = all_cells[:, 0]

        all_cells_with_values[:, 2] = convert[mask[all_cells[:, 0], all_cells[:, 1]]]

        segmented_image = create_cell_segmentation_image(
            annotated_data=all_cells_with_values, 
            cell_mpp=0.2,
            # radius=1.7
        )

        maps.append(segmented_image)

    return torch.tensor(maps).permute(0, 3, 1, 2)


def combine_image_tensors(tensor1, tensor2):
    """
    Combines two image tensors with specific rules for red, green, and blue pixels.

    For each pixel:
    - The pixel is red if both corresponding pixels in tensor1 and tensor2 are red.
    - If the pixel in tensor1 is not red, it takes the value of tensor1.
    - If the pixel in tensor1 is red, it takes the value of tensor2.

    Parameters:
    tensor1 (numpy.ndarray): The first image tensor with shape (channels, height, width).
    tensor2 (numpy.ndarray): The second image tensor with shape (channels, height, width).

    Returns:
    numpy.ndarray: A new image tensor formed by combining tensor1 and tensor2.
    """

    if tensor1.shape != tensor2.shape:
        raise ValueError("The dimensions of the two tensors must be the same")

    combined_tensor = torch.zeros_like(tensor1)

    for i in range(tensor1.shape[1]):
        for j in range(tensor1.shape[2]):
            if torch.all(tensor1[:, i, j] == torch.tensor([1, 0, 0])) and torch.all(tensor2[:, i, j] == torch.tensor([1, 0, 0])):
                # Both pixels are red
                combined_tensor[:, i, j] = torch.tensor([1, 0, 0])
            elif not torch.all(tensor1[:, i, j] == torch.tensor([1, 0, 0])):
                # Pixel in tensor1 is not red
                combined_tensor[:, i, j] = tensor1[:, i, j]
            else:
                # Pixel in tensor1 is red, use pixel from tensor2
                combined_tensor[:, i, j] = tensor2[:, i, j]

    return combined_tensor


def translate_color(tensor, color_to_translate, translated_color):
    """
    Translates a color in the image tensor to another color.

    Parameters:
    tensor (numpy.ndarray): Image tensor with shape (channels, height, width).
    color_to_translate: Color to be translated.
    translated_color: new color after translation.

    Returns:
    numpy.ndarray: Modified image tensor.
    """

    modified_tensor = torch.clone(tensor)

    for i in range(tensor.shape[1]):
        for j in range(tensor.shape[2]):
            # Check if the pixel is green
            if torch.all(tensor[:, i, j] == torch.tensor(color_to_translate)):
                modified_tensor[:, i, j] = torch.tensor(translated_color)

    return modified_tensor

def calculate_dice_score(dataloader, model, device): 

    dice_metric = DiceMetric(include_background=True, reduction="mean")
    # post_pred = Compose([AsDiscrete(argmax=True, dim=1, to_onehot=3)])

    model.eval()
    for (images, masks) in dataloader: 
        images = images.to(device)
        with torch.no_grad(): 
            outputs = model(images)
        
        outputs_cpu = outputs.cpu()
        # outputs_cpu = post_pred(outputs_cpu)

        outputs = outputs_cpu #.to(device)

        outputs = make_prediction_map(outputs)
        # outputs = outputs.to(device)

        dice_score = dice_metric(outputs, masks)


        mask = translate_color(masks[0].squeeze(), [0, 1, 0], [1, 1, 0])
        mask = translate_color(mask, [0, 0, 1], [1, 0, 1])
        # mask = translate_color(mask, [1, 0, 0], [1, 1, 1])

        # Cancer: green, yellow
        # Regular: Blue, purple
        
        combination_tensor = combine_image_tensors(mask, outputs[0].squeeze())

        # Plotting tissue image
        tissue_image = images[0][3].cpu()

        plot_over_image = combine_image_tensors(combination_tensor, images[0][:3].cpu())

        plt.figure(figsize=(24, 12))

        # Uncomment and adjust the subplot parameters to minimize white space
        plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, wspace=0.05, hspace=0.05)

        plt.subplot(3, 2, 1)
        plt.imshow((tissue_image*255).to(torch.uint8))
        plt.title("Tissue classification")
        plt.axis("off")

        plt.subplot(3, 2, 2)
        plt.imshow((outputs_cpu[0].squeeze().permute(1, 2, 0)*255).to(torch.uint8))
        plt.title("Predictions")
        plt.axis("off")

        plt.subplot(3, 2, 3)
        plt.imshow(outputs[0].squeeze().permute(1, 2, 0)*255)
        plt.title("predictions peak")
        plt.axis("off")

        plt.subplot(3, 2, 4)
        plt.imshow(torch.tensor(combination_tensor).permute(1, 2, 0))
        plt.title("ground truth in combination with predicton")
        plt.axis("off")

        plt.subplot(3, 2, 5)
        plt.imshow(torch.tensor(plot_over_image).permute(1, 2, 0))
        plt.title("ground truth in combination with predicton over image")
        plt.axis("off")

        plt.subplot(3, 2, 6)
        plt.imshow(mask.permute(1, 2, 0))
        plt.title("ground truth")
        plt.axis("off")

        plt.show()

        print("Dice score:", dice_score)
        
        # plt.figure(figsize=(16, 8))
        # plt.subplot(1, 3, 1)
        # plt.imshow((outputs_cpu[1].squeeze().permute(1, 2, 0)*255).to(torch.uint8))
        # plt.title("predictions")
        # plt.axis("off")

        # plt.subplot(1, 3, 2)
        # plt.imshow(outputs[1].squeeze().permute(1, 2, 0)*255)
        # plt.title("predictions peak")
        # plt.axis("off")

        # plt.subplot(1, 3, 3)
        # plt.imshow(masks[1].squeeze().permute(1, 2, 0))
        # plt.title("ground truth")
        # plt.axis("off")
        # plt.show()

    dice_score = dice_metric.aggregate().item()
    dice_metric.reset()

    return dice_score
```

```{python}
import numpy as np
import torch
from skimage.feature import peak_local_max

def calculate_dice_score(dataloader, model, device):
    model.eval()
    total_dice_scores = []
    pixel_radius = 1.4 / 0.2
    search_radius = int(3 / 0.2)

    for (images, masks) in dataloader:
        images = images.to(device)
        with torch.no_grad():
            outputs = model(images)

        outputs_cpu = outputs.cpu()
        for i in range(outputs_cpu.shape[0]):
            output = outputs_cpu[i]
            mask = masks[i]
            _, ground_truth = torch.max(mask, axis=0)

            softmaxed = torch.softmax(output, dim=0)
            cells, argmaxed = torch.max(softmaxed, axis=0)
            argmaxed = argmaxed.cpu().numpy()
            cells = cells.cpu().numpy()

            peak_points_pred = peak_local_max(cells, min_distance=20, labels=np.logical_or(argmaxed == 1, argmaxed == 2), threshold_abs=0.01)

            TP = 0
            FP = 0
            for pred_point in peak_points_pred:
                pred_class = argmaxed[pred_point[0], pred_point[1]]
                # gt_cell = ground_truth[pred_point[0], pred_point[1]]

                # if pred_class == gt_cell:
                #     TP += 1

                ## Spatial check within a radius
                TP_tmp = TP
                for dx in range(-search_radius, search_radius + 1):
                    if TP > TP_tmp:
                        break
                    for dy in range(-search_radius, search_radius + 1):
                        x, y = pred_point[0] + dx, pred_point[1] + dy
                        if 0 <= x < ground_truth.shape[0] and 0 <= y < ground_truth.shape[1]:
                            gt_class = ground_truth[x, y].item()
                            if pred_class == gt_class and (dx**2 + dy**2) <= search_radius**2:
                                TP += 1
                                break  # Stop checking other points in the radius for this pred_point

                if TP == TP_tmp:
                    FP += 1

            num_gt_cells = (np.logical_or(ground_truth==1, ground_truth==2)).sum().item() // (np.pi * pixel_radius**2) 
            
            num_pred_cells = len(peak_points_pred)

            FN = num_gt_cells - (TP + FP)

            if num_gt_cells + num_pred_cells > 0:
                dice_score = (2.0 * TP) / (2*TP + FP + FN)
                total_dice_scores.append(dice_score)
            else:
                total_dice_scores.append(1)  # Both empty, perfect match

    average_dice_score = np.mean(total_dice_scores) if total_dice_scores else 0

    return average_dice_score

```

```{python}
def calculate_f1_score(
    model, dataloader, device, micron_radius: float = 3.0, mpp: float = 0.2
):
    """These are the steps for calculating the F1 score, as given in the paper:

    - True positive: If a detected cell is within a valid distance (3 microns) of a
      target cell, then it is considered a TP
    - False positive: If a detected cell does not fulfill the requirements for a
      TP, then it is considered a FP
    - False Negative: If an annotated cell is not detected, then it is counted as a
      False Negative

    """
    model.eval()
    f1_scores = []
    pixel_radius = micron_radius / mpp
    dataset = dataloader.dataset
    batch_size = dataloader.batch_size

    for batch_idx, (image_batch, mask_batch) in enumerate(dataloader):
        image_batch = image_batch.to(device)
        with torch.no_grad():
            output_batch = model(image_batch)

        for idx in range(output_batch.shape[0]):
            output = output_batch[idx]
            # mask = mask_batch[idx].permute(1, 2, 0)

            image_no = batch_idx * batch_size + idx
            cell_annotation_list = dataset.get_cell_annotation_tensor(image_no)

            # Preparing output for peak_local_max
            softmaxed = torch.softmax(output, dim=0)
            cells, argmaxed = torch.max(softmaxed, axis=0)
            argmaxed = argmaxed.cpu().numpy()
            cells = cells.cpu().numpy()
            peak_points_pred = peak_local_max(
                cells,
                min_distance=20,
                labels=np.logical_or(argmaxed == 1, argmaxed == 2),
                threshold_abs=0.01,
            )

            
            # # For plotting the predictions and the ground truth on top
            
            # output = make_prediction_map(output_batch)[0]*255
            
            # # display results
            # fig, ax = plt.subplots(1, 1, figsize=(8, 3), sharex=True, sharey=True)

            # ax.imshow(output.permute(1, 2, 0))
            # ax.autoscale(False)
            # ax.plot(cell_annotation_list[:, 0], cell_annotation_list[:, 1], 'o', color='black', markersize=0.5)
            # ax.axis('off')
            # ax.set_title('Peak local max')

            # fig.tight_layout()

            # plt.show()


            # We want to compare the masks to the outputs to calculate TP, FP and FN
            # First, we have to find the points from the outputs using peak_local_max
            TP = 0
            FP = 0
            TP_tmp = 0
            FP_tmp = 0
            
            for y, x in peak_points_pred:
                # We check a circle around the point to see if there is a cell in the mask
                # If there is, we count it as a TP
                cell_type = argmaxed[y, x]
                
                TP_tmp = TP
                FP_tmp = FP

                # for i, cell in enumerate(cell_annotation_list[
                #     cell_annotation_list[:, 2] != cell_type # Important with unequal because cancer coded differently
                # ]):
                for i, cell in enumerate(cell_annotation_list):
                    distance_squared = (x - cell[0]) ** 2 + (y - cell[1]) ** 2
                    if distance_squared <= pixel_radius**2:
                        if cell[2] != cell_type: # Important with unequal because cancer coded differently
                          TP += 1
                        else:
                          FP += 1
                        # Removing the found cell
                        mask = torch.arange(cell_annotation_list.size(0)) != i
                        cell_annotation_list = cell_annotation_list[mask]
                        break
                    
                # print(argmaxed[x, y])
                # print(np.unique(argmaxed))
                # print(mask.shape)
                # print(mask[x, y])
                if TP_tmp == TP and FP_tmp == FP:
                  FP += 1
          
            FN = len(cell_annotation_list)
                
            dice_score = (2.0 * TP) / (2*TP + FP + FN)
            f1_scores.append(dice_score)

            # print(TP, FP, FN)

            # if len(f1_scores) == 2:
            #     return f1_scores
  
    return torch.mean(torch.tensor(f1_scores))
```

```{python}
backbone = "resnet50"
dropout_rate = 0.3
model = _segm_resnet(
    name="deeplabv3plus",
    backbone_name=backbone,
    num_classes=3,
    output_stride=8,
    pretrained_backbone=True,
    dropout_rate=dropout_rate,
    num_channels=4,
    input_tissue_v3=True
)
model.to(device) 
# model.load_state_dict(torch.load("outputs/models/2023-12-07_17-04-08_deeplabv3plus_tissue_leaking_lr-0.0001_dropout-0.3_backbone-resnet50_epochs-300.pth"))
model.load_state_dict(torch.load("outputs/models/2024-01-15_14-32-57_deeplabv3plus_tissue_leaking_lr-1e-05_dropout-0.3_backbone-resnet50_epochs-300.pth"))
model.eval()
```

```{python}
test_dice_score = calculate_f1_score(model, test_dataloader, device)
val_dice_score = calculate_f1_score(model, val_dataloader, device)

print(f"Dice score on test set: {test_dice_score}")
print(f"Dice score on validation set: {val_dice_score}")

# Tissue fed in ad the fourth dimension to resnet
# outputs/models/2023-12-07_17-04-08_deeplabv3plus_tissue_leaking_lr-0.0001_dropout-0.3_backbone-resnet50_epochs-300.pth
# Dice score on test set: 0.725593626499176
# Dice score on validation set: 0.7120229601860046


# Tissue fed deeplabv3. Both tissue and cell info is encoded with resnet. ResNet not freezed
# outputs/models/2024-01-15_14-32-57_deeplabv3plus_tissue_leaking_lr-1e-05_dropout-0.3_backbone-resnet50_epochs-300.pth
# Dice score on test set: 0.6813595294952393
# Dice score on validation set: 0.6469401717185974





```

```{python}
test_dice_score = calculate_dice_score(test_dataloader, model, device)
# val_dice_score = calculate_dice_score(val_dataloader, model, device)

test_dice_score = calculate_f1_score(model, test_dataloader, device, micron_radius=10)

print("Tissue Leaking model, lr=0.0001, dropout=0.3, backbone=resnet34")
print(f"Dice score on test set: {test_dice_score}")
# print(f"Dice score on validation set: {val_dice_score}")


# 100 epochs:
# Dice score on test set: 0.6255452632904053
# Dice score on validation set: 0.6431155204772949

# 70 epochs:
# Dice score on test set: 0.6230767965316772
# Dice score on validation set: 0.642177402973175

# 130 epochs:
# Dice score on test set: 0.6108373999595642
# Dice score on validation set: 0.6346875429153442

# 300 epochs:
# Dice score on test set: 0.6164279580116272
# Dice score on validation set: 0.6312710642814636


# 250 without pretrain:
# Dice score on test set: 0.5942952632904053
# Dice score on validation set: 0.611300528049469

# 300 without pretrain:
# Dice score on test set: 0.5951597094535828
# Dice score on validation set: 0.6107892990112305

# 300 without pretrain and with resnet34
# Dice score on test set: 0.6059926152229309
# Dice score on validation set: 0.6229147911071777


## Pretraied backbone for images. Images and tissue fed to deeplabv3
# 100 epochs resnet50
# Dice score on test set: 0.6108450293540955
# Dice score on validation set: 0.6313456892967224


# Search radius: 48
# Dice score on test set: 0.6523898229055771
# Dice score on validation set: 0.6066405512355199
```

```{python}
import torch.nn.functional as F
from skimage.feature import peak_local_max

from src.utils import create_cell_segmentation_image

img, seg = next(iter(test_dataloader))
img, seg = img.to(device), seg.to(device)

model.eval()
with torch.no_grad():
    output = model(img)
probabilities = F.softmax(output, dim=1)
print(probabilities.size())
```

```{python}
argmaxed = torch.argmax(output, dim=1)
plt.imshow(argmaxed[0].detach().cpu())
plt.show()
print(argmaxed.size())
```

```{python}
cells = np.max(softmax(output.cpu()[1], 0).numpy(), axis=0)
print(cells.shape)
# print(cells[:, 1, 1])
```

```{python}
mask = argmaxed[0].cpu().numpy()

# tumor_cells = peak_local_max(cells, min_distance=20, labels=cells == 2)
# background_cells = peak_local_max(cells, min_distance=20, labels=cells == 1)

all_cells = peak_local_max(cells, min_distance=20, labels=np.logical_or(mask==1, mask==2))

# background_cells_with_labels = [(x, y, 2) for x, y in background_cells]
# tumor_cells_with_labels = [(x, y, 1) for x, y in tumor_cells]

# coords_with_labels = background_cells_with_labels + tumor_cells_with_labels
# coords_with_labels_tensor = torch.tensor(coords_with_labels)
```

```{python}
all_cells_with_values = np.empty((all_cells.shape[0], 3), dtype=all_cells.dtype)

all_cells_with_values[:, 0] = all_cells[:, 1]
all_cells_with_values[:, 1] = all_cells[:, 0]

all_cells_with_values[:, 2] = mask[all_cells[:, 0], all_cells[:, 1]]

# print(all_cells_with_values)
# print(all_cells_with_values[:, :2] == all_cells)
```

```{python}
segmented_image = create_cell_segmentation_image(
    annotated_data=all_cells_with_values, 
    cell_mpp=0.2
)
```

```{python}
segmented_image.shape
```

```{python}
segmented_image = make_prediction_map(output[0].unsqueeze(0)).squeeze().permute(1, 2, 0)
plt.imshow(segmented_image*255)
plt.show()
plt.imshow(seg.cpu()[0].permute(1, 2, 0))
plt.show()
```

```{python}
input_image = img[0, :3].cpu()
plt.figure(figsize=(12, 4))
#plt.scatter(background_cells[:, 1], background_cells[:, 0], s=7, facecolors="none", edgecolors="y")
#plt.scatter(tumor_cells[:, 1], tumor_cells[:, 0], s=7, facecolors="none", edgecolors="g")


plt.subplot(1, 4, 1)
plt.xlim(0, 1024)
plt.ylim(0, 1024)
plt.gca().invert_yaxis()
plt.imshow(segmented_image*255)
plt.title("Peak local max")
plt.axis("off")


plt.subplot(1, 4, 2)
plt.scatter(all_cells[:, 1], all_cells[:, 0], s=7, facecolors="none", edgecolors="y")
plt.xlim(0, 1024)
plt.ylim(0, 1024)
plt.gca().invert_yaxis()
plt.imshow(argmaxed[0].detach().cpu())
plt.title("Argmaxed model output\n + peak_local_max")
plt.axis("off")

plt.subplot(1, 4, 3)
plt.imshow(seg.cpu()[0].permute(1, 2, 0))
#plt.imshow(argmaxed[0].detach().cpu())
plt.scatter(all_cells_with_values[:, 0], all_cells_with_values[:, 1], s=7, facecolors="none", edgecolors="black")
plt.title("Target segmentation mask \n+ peak_local_max of outputs")
plt.axis("off")

plt.subplot(1, 4, 4)
plt.imshow(input_image.permute(1, 2, 0))
plt.title("Input image")
plt.axis("off")
plt.tight_layout()
plt.show()
```

```{python}
plt.plot()
plt.imshow(seg.cpu()[0].permute(1, 2, 0))
#plt.imshow(argmaxed[0].detach().cpu())
plt.scatter(all_cells_with_values[:, 0], all_cells_with_values[:, 1], s=7, facecolors="none", edgecolors="black")
plt.title("Target segmentation mask \n+ peak_local_max of outputs")
plt.axis("off")
```

```{python}
training_losses = [0.6841486388323258, 0.6212089378006604, 0.5801500349628682, 0.5495993154389518, 0.5279010005143224, 0.5205534854713751, 0.5152575142529546, 0.5087966401966251, 0.5016141205417867, 0.5007373988628387, 0.4967713666205503, 0.4958431349725139, 0.49552888347178087, 0.49398842818883, 0.4911923749106271, 0.48889031033126673, 0.48940872598667534, 0.49012379135404316, 0.4894666556192904, 0.48915101253256504, 0.4851855866763057, 0.484701296504663, 0.4804936933274172, 0.48094433424424154, 0.4792345756170701, 0.4802650857944878, 0.4777539560989458, 0.47960659374996106, 0.47847011137981804, 0.47665113453962366, 0.47783021963372524, 0.47705443537965114, 0.47590979872917644, 0.47520980299735555, 0.4726546187790073, 0.4724834755975373, 0.4713993802362559, 0.4697963765689305, 0.4721752611958251, 0.47213491310878675, 0.47415965248127373, 0.47092501484617894, 0.4717656970024109, 0.4701739348927323, 0.469322388877674, 0.4683166438219499, 0.465940997916825, 0.4653265543129979, 0.46737044623919893, 0.4680443959576743, 0.46518668958118986, 0.4626572247670621, 0.46380412456940634, 0.4641559154403453, 0.4648526055472238, 0.4612105233328683, 0.4642008311894475, 0.46511735903973483, 0.4619367925488219, 0.46032845183294646, 0.4606601583714388, 0.4606500693729946, 0.4591324980161628, 0.4587999101804227, 0.4606908267858077, 0.45913051464119736, 0.4584291346219121, 0.45868738634245737, 0.45783284610631514, 0.4578557154353784, 0.45774753178868977, 0.457902512380055, 0.454241224697658, 0.45930349036138884, 0.4555462045328958, 0.4544881947186528, 0.45549969892112574, 0.454612173596207, 0.4556689894929224, 0.4520343980010675, 0.4533381553328767, 0.45030100734866396, 0.45077827147075106, 0.4499681561577077, 0.45199426522060315, 0.4509895887910103, 0.448741537575819, 0.4459245375224522, 0.44674587614682254, 0.44898361028457173, 0.44786570753370014, 0.4458292558485148, 0.4454021502514275, 0.44390675364708415, 0.44555747995571215, 0.4442661830357143, 0.4466289360912479, 0.4457537580509575, 0.43951864753450665, 0.4421841483943316, 0.4444117527835223, 0.4449098402140092, 0.4437132508170848, 0.437016419001988, 0.43965906512980557, 0.44104473505701336, 0.4404640434956064, 0.4406539372035435, 0.4417357231889452, 0.4418514498642513, 0.44009047625016195, 0.4364860021338171, 0.44045475003670675, 0.44010625384291824, 0.43894074340255895, 0.4379192669780887, 0.4353175570770186, 0.4386143179572358, 0.4366673054743786, 0.4361391420267066, 0.4349785878950236, 0.43574444736753193, 0.43036121008347494, 0.4368141074569858, 0.4328059237830493, 0.430777946905214, 0.4306173464473413, 0.43127344518291705, 0.4319693926645785, 0.4293568371510019, 0.42852924794566877, 0.42638077723736667, 0.42789738336387945, 0.42534187983493416, 0.43049160071781706, 0.42722421884536743, 0.42247933818369493, 0.4188290268790965, 0.408616661417241, 0.3705833913112173, 0.3688103149131853, 0.3562250788114509, 0.3507631415007066, 0.35519666270333894, 0.3607778193391099, 0.35217223544510046, 0.35161066024887316, 0.3516391172092788, 0.3489526443335475, 0.3527535163626379, 0.34857464079954187, 0.3554913328618419, 0.3584876291605891, 0.35270476584531824, 0.35557716780779314, 0.3450013317015706, 0.3430341415259303, 0.343488482796416, 0.34384236439150206, 0.3464021694903471, 0.34960884099103967, 0.3456262897471992, 0.35260104585667046, 0.3538413102529487, 0.3489852127980213, 0.3405558290530224, 0.3417405887525909, 0.3602482037884848, 0.3577935850741912, 0.34471612317221506, 0.3440142581049277, 0.3405612372622198, 0.34127302923981023, 0.34646865543054073, 0.3454240797733774, 0.35705889852679507, 0.352864608168602, 0.34192392017160145, 0.33960468246012315, 0.3427189229702463, 0.3414616615188365, 0.33478899938719614, 0.3392052200375771, 0.3461189549796435, 0.35023995929834795, 0.3455998258931296, 0.3347379239846249, 0.3383765470008461, 0.33499879435617097, 0.3439928278022883, 0.3503279807616253, 0.3407889945166452, 0.3416167552373847, 0.35277619714639624, 0.3525145357968856, 0.35122692402528255, 0.34343066598687855, 0.341192683090969, 0.3376895164956852, 0.3374565450512633, 0.33589436874097706, 0.34275864490440916, 0.3559067383104441, 0.3463086266906894, 0.3360385088896265, 0.3377751379596944, 0.3453568268795403, 0.3538357293119236, 0.33862097074790876, 0.33348353876143083, 0.3371021531674327, 0.36644746576036724, 0.3575176031005626, 0.3459223332453747, 0.3403852426884126, 0.3386810348958385, 0.35707468585092195, 0.35060789603359843, 0.34185839337962015, 0.34552600067489003, 0.3425825110503605, 0.33309905808799123, 0.33155034482479095, 0.33182530013882383, 0.331008219597291, 0.33217903424282463, 0.3308140580751458, 0.3274856130687558, 0.32827942073345184, 0.3275855779647827, 0.3410742042624221, 0.36200983913577334, 0.3402498279299055, 0.34552307153234674, 0.3481884635224634, 0.34119353853926365, 0.3365509540450816, 0.3347735325900876, 0.32682872061826745, 0.3253616848770453, 0.3418063022652451, 0.34198180844589154, 0.3372370518591939, 0.32980654738387283, 0.3293382197010274, 0.3280854602249301, 0.32729701484952656, 0.32597906127267956, 0.3338408038324239, 0.3268376795613036, 0.3338105033855049, 0.3268966531875182, 0.32586003718327505, 0.3282929832229809, 0.33072708729578526, 0.3339989443822783, 0.32933749288928754, 0.32286655173009754, 0.3186792150443914, 0.3208051460738085, 0.32002996394828875, 0.32242203610283987, 0.3195406907067007, 0.3198195583358103, 0.32267503349148496, 0.330596136195319, 0.3226987713453721, 0.3307507077650148, 0.322461729755207, 0.3236000422312289, 0.31986171949882897, 0.3197466642880926, 0.3173269197648885, 0.31948394526024254, 0.3184876466283993, 0.3180225427661623, 0.32021276591991893, 0.32346582534361856, 0.31493155536602957, 0.31476317771843504, 0.31482222220119166, 0.3156180038135879, 0.3119163364171982, 0.3150407647599979, 0.32046326812432735, 0.3184512208919136, 0.34279283516261044, 0.3495277288008709, 0.3576833143526194, 0.3381782101125133, 0.33995632188660757, 0.32591853579696345, 0.32308170990068086, 0.32034171935246913, 0.32022613165329916, 0.3256206551984865, 0.3237351541008268, 0.318005946217751, 0.3169619732973527, 0.32419392953113635]
val_losses = [0.640541136264801, 0.5727361265350791, 0.519561346839456, 0.5000012166359845, 0.48358149388257193, 0.48845182446872487, 0.48516253162832823, 0.47146080521976247, 0.47466253182467294, 0.46822962866109963, 0.46768983672646913, 0.4691689329988816, 0.46342217571595135, 0.46757371986613555, 0.4717768465771395, 0.4631367343313554, 0.4610307181582731, 0.46002232853104086, 0.464755315991009, 0.4575614087721881, 0.45633027834050793, 0.4539649363826303, 0.4601457609849818, 0.45615970562486086, 0.45437761439996605, 0.4535137204562916, 0.453689867959303, 0.45724502205848694, 0.45451372335938844, 0.45289908787783456, 0.4566147432607763, 0.45574015729567585, 0.4536086092976963, 0.45512012348455544, 0.4518888189512141, 0.44948946903733644, 0.4521763342268327, 0.45655417267014, 0.4524749079171349, 0.45214633906588836, 0.45326519888990063, 0.4510942890363581, 0.45126154142267566, 0.4516636098132414, 0.4510260662611793, 0.4505048657164854, 0.45024674955536337, 0.4506105990970836, 0.4481393046238843, 0.44740090650670666, 0.44702398952315836, 0.4479106156265034, 0.45334066362941966, 0.4525732836302589, 0.45238320967730355, 0.4522404337630552, 0.44596786358777213, 0.4503253505510442, 0.44733916310703054, 0.4518462279263665, 0.44979871546520905, 0.457413128193687, 0.4495520241120282, 0.4547430487240062, 0.4458735918297487, 0.446075579699348, 0.45136188934831056, 0.44496632849468903, 0.44610974367927103, 0.44971972528625936, 0.4479798814829658, 0.4469507441801183, 0.4531952160246232, 0.447667097344118, 0.4510014039628646, 0.4457736734081717, 0.45108271521680493, 0.4480328612467822, 0.44965246144463034, 0.44993375855333667, 0.4455918073654175, 0.4464574543868794, 0.448580075712765, 0.4536908058559193, 0.44962820936651793, 0.44783081903177147, 0.44574199704562917, 0.4489367358824786, 0.45045673847198486, 0.451046463321237, 0.45213667610112357, 0.4494233902762918, 0.4538958879078136, 0.4482164435526904, 0.4499651702011333, 0.4505189148818745, 0.4471667479066288, 0.4546267092227936, 0.4500228359418757, 0.4489220345721525, 0.45254727496820335, 0.4564576972933376, 0.4487612229936263, 0.4509342330343583, 0.45510218073340025, 0.4501059476067038, 0.4494169880362118, 0.4520138063851525, 0.4524785052327549, 0.4485899823553422, 0.44546131526722627, 0.4507138571318458, 0.453203501070247, 0.4506346688551061, 0.4569861994070165, 0.4545397355275996, 0.44983819653006163, 0.46356672749799843, 0.45309096574783325, 0.4526856103364159, 0.45628000006956215, 0.4540201618390925, 0.4519096051945406, 0.4614884818301481, 0.45154350820709677, 0.45299125769559073, 0.45738380621461305, 0.45348282246028676, 0.45077721511616425, 0.45718906556858735, 0.4557149199878468, 0.4597352164633134, 0.4490214533665601, 0.45428957308039947, 0.4469674229621887, 0.4546486910651712, 0.454109333893832, 0.43254193838904886, 0.39196109421112957, 0.3619166822994457, 0.3570322622271145, 0.3582286571755129, 0.3633854932644788, 0.3692023719058317, 0.36181707241955924, 0.3591980285504285, 0.3647316624136532, 0.3651086617918575, 0.3612010461442611, 0.3782190943465513, 0.3707213033648098, 0.3649942647008335, 0.35971983741311464, 0.37508310991175037, 0.36482242275686827, 0.36706814520499287, 0.3710976470919216, 0.3737148239332087, 0.3643339118536781, 0.36361493082607493, 0.3682505435803357, 0.3747693738516639, 0.3753199174123652, 0.3724367075106677, 0.3754640712457545, 0.36870461176423464, 0.3685702681541443, 0.37423787222189064, 0.37091739037457633, 0.36691319942474365, 0.36445549831670876, 0.3691141640438753, 0.3629386109464309, 0.3676681588677799, 0.37745431065559387, 0.37848049928160277, 0.36893715753274803, 0.37184808534734387, 0.3682524316451129, 0.3745453392758089, 0.3660202902906081, 0.36211000821169687, 0.3747788492371054, 0.3724240152274861, 0.3694510950761683, 0.3640609488767736, 0.3656996330794166, 0.3707975850385778, 0.36203039218397703, 0.38401920304578896, 0.36144134226967306, 0.37238574028015137, 0.37418561472612266, 0.367951396633597, 0.3736901476102717, 0.37313954619800344, 0.375934956704869, 0.3627842945211074, 0.3677073646994198, 0.3728902725612416, 0.3647316185867085, 0.3819349776296055, 0.366142213344574, 0.3692800595479853, 0.36844289478133707, 0.362356540034799, 0.36756376133245583, 0.3781612305080189, 0.3657262395409977, 0.368922166964587, 0.3713926287258373, 0.37660305464968963, 0.36478717888102813, 0.3666432499885559, 0.3634117999497582, 0.36365098111769734, 0.37323848114294167, 0.36859617338461037, 0.3757902296150432, 0.37483564019203186, 0.3650486697168911, 0.36982446032411914, 0.3695067921105553, 0.3652686108561123, 0.3628951381234562, 0.36880770851584044, 0.368797712466296, 0.3677140614565681, 0.3701018743655261, 0.36951981397236094, 0.38613880557172436, 0.364570009357789, 0.36869915092692657, 0.3720579305115868, 0.3674858790986678, 0.36326930978718924, 0.3665287371943979, 0.3684387154438916, 0.36720967468093424, 0.3694309855208677, 0.36434431812342477, 0.3704874971333672, 0.36685388579088096, 0.36772024806808024, 0.36720016774009256, 0.371899252428728, 0.3639393378706539, 0.3652030965861152, 0.37091147023088794, 0.3660519999616286, 0.37087414194555846, 0.36359158508917866, 0.3685559823232539, 0.36583885375191183, 0.3632868142688976, 0.366905037094565, 0.3685731835225049, 0.3660558514735278, 0.36681286201757546, 0.3659642005667967, 0.36505666901083555, 0.36715805881163655, 0.3646949985448052, 0.3677670236896066, 0.3704529264393975, 0.3690778507905848, 0.3660057993496166, 0.3729998609598945, 0.3647821615724003, 0.3679232947966632, 0.3668749122058644, 0.3659541186164407, 0.3642602825866026, 0.37125641107559204, 0.3760099568787743, 0.37080951473292184, 0.3716816411298864, 0.3690169278313132, 0.36340593590455894, 0.3691788824165569, 0.36706527892281027, 0.366748662555919, 0.37648042861153097, 0.3661031126976013, 0.36498723135275, 0.37297209045466256, 0.36542654212783365, 0.3765753192060134, 0.36155332361950593, 0.36728781812331257, 0.3716471510774949, 0.36695749268812294, 0.3704073253799887, 0.37359897529377656, 0.3681257857995875, 0.3738099266501034, 0.372946115101085, 0.3685232102870941, 0.378370100961012, 0.36646268472952004]
plt.figure(figsize=(20, 6))
plt.plot(training_losses, label="Training Loss")
plt.plot(val_losses, label="Validation Loss")
plt.title("Training and Validation Loss for experiment 2")
plt.xlabel("Number of epochs")
plt.ylabel("Dice Loss")
plt.legend()
plt.show()
```
