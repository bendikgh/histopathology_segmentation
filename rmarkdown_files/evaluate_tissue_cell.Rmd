---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: specialization_project
    language: python
    name: python3
---

```{python}
# Fixing automatic autoreload
# %load_ext autoreload
# %autoreload 2
```

```{python}
import os 

# Making sure we are running the code from the root directory
current_directory = os.getcwd()
if current_directory.endswith("notebooks"):
    os.chdir("..")
    print("Changed directory to:", os.getcwd())
else:
    print("Directory was already correct, so did not change.")
```

```{python}
import albumentations as A
import cv2
import matplotlib.pyplot as plt
import torch
import numpy as np
import seaborn as sns

from glob import glob
from monai.data import DataLoader

from src.deeplabv3.network.modeling import _segm_resnet
from src.models import DeepLabV3plusModel
from src.dataset import CellTissueDataset
from src.utils.metrics import calculate_f1_score
from torch.nn.functional import one_hot 

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"device: {device}")
sns.set_theme()
```

```{python}
data_path = "/cluster/projects/vc/data/mic/open/OCELOT/ocelot_data"

train_seg_files = glob(os.path.join(data_path, "annotations/train/segmented_cell/*"))
train_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in train_seg_files
]
train_image_files = [
    os.path.join(data_path, "images/train/cell", image_number + ".jpg")
    for image_number in train_image_numbers
]

val_seg_files = glob(os.path.join(data_path, "annotations/val/segmented_cell/*"))
val_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in val_seg_files
]
val_image_files = [
    os.path.join(data_path, "images/val/cell", image_number + ".jpg")
    for image_number in val_image_numbers
]

test_seg_files = glob(os.path.join(data_path, "annotations/test/segmented_cell/*"))
test_image_numbers = [
    file_name.split("/")[-1].split(".")[0] for file_name in test_seg_files
]
test_image_files = [
    os.path.join(data_path, "images/test/cell", image_number + ".jpg")
    for image_number in test_image_numbers
]
```

```{python}
val_tissue_seg = []

for img_path in val_seg_files:
    ending = img_path.split("/")[-1].split(".")[0]
    tissue_seg_path = glob(
        os.path.join(data_path, "annotations/val/cropped_tissue/" + ending + "*")
    )[0]
    input_img_path = glob(
        os.path.join(data_path, "images/val/cell/" + ending + "*")
    )[0]
    val_tissue_seg.append(tissue_seg_path)
```

```{python}
train_tissue_predicted = glob(os.path.join(data_path, "annotations/train/pred_tissue/*"))
val_tissue_predicted = glob(os.path.join(data_path, "annotations/val/pred_tissue/*"))
test_tissue_predicted = glob(os.path.join(data_path, "annotations/test/pred_tissue/*"))
```

```{python}
batch_size = 1

train_cell_tissue_dataset = CellTissueDataset(
    cell_image_files=train_image_files,
    cell_target_files=train_seg_files,
    tissue_pred_files=train_tissue_predicted
)
val_cell_tissue_dataset = CellTissueDataset(
    cell_image_files=val_image_files,
    cell_target_files=val_seg_files,
    tissue_pred_files=val_tissue_predicted
)
test_cell_tissue_dataset = CellTissueDataset(
    cell_image_files=test_image_files,
    cell_target_files=test_seg_files,
    tissue_pred_files=test_tissue_predicted,
)

train_cell_tissue_dataloader = DataLoader(
    dataset=train_cell_tissue_dataset, batch_size=batch_size, drop_last=True
)
val_cell_tissue_dataloader = DataLoader(
    dataset=val_cell_tissue_dataset, batch_size=batch_size
)
test_cell_tissue_dataloader = DataLoader(
    dataset=test_cell_tissue_dataset, batch_size=batch_size
)
```

```{python}
val_tissue_dataset = CellTissueDataset(
    cell_image_files=val_image_files,
    cell_target_files=val_seg_files,
    tissue_pred_files=val_tissue_seg,
)
```

```{python}
backbone = "resnet50"
dropout_rate = 0.3
tissue_cell_model = DeepLabV3plusModel(
    backbone_name="resnet50",
    num_classes=3,
    num_channels=6,
    pretrained=True,
    dropout_rate=dropout_rate,
)
tissue_cell_model.to(device)
tissue_cell_model.load_state_dict(
    torch.load(
        "outputs/models/20240228_192603_deeplabv3plus-cell-branch_pretrained-1_lr-5e-05_dropout-0.3_backbone-resnet50_epochs-100.pth"
    )
)
tissue_cell_model.eval()
print()
```

```{python}
backbone = "resnet50"
dropout_rate = 0.3
tissue_model = DeepLabV3plusModel(
    backbone_name="resnet50",
    num_classes=3,
    num_channels=3,
    pretrained=True,
    dropout_rate=dropout_rate,
)
tissue_model.to(device)
tissue_model.load_state_dict(
    torch.load(
        "outputs/models/20240228_192603_deeplabv3plus-tissue-branch_pretrained-1_lr-5e-05_dropout-0.3_backbone-resnet50_epochs-100.pth"
    )
)
tissue_model.eval()
print()
```

```{python}
# val_score = calculate_f1_score(model=tissue_cell_model, dataloader=val_cell_tissue_dataloader, device=device)
# print(f"Validation score: {val_score}")
# test_score = calculate_f1_score(model=tissue_cell_model, dataloader=test_cell_tissue_dataloader, device=device)
# print(f"Test score: {test_score}")
```

```{python}
index = 4

image, label_cell = val_cell_tissue_dataset[index]

file_name = val_cell_tissue_dataset.cell_image_files[index]
num = file_name.split("/")[-1].split(".")[0]

val_tissue_image_path = os.path.join(data_path, "images", "val", "tissue", num + ".jpg")
val_tissue_ground_truth_path = os.path.join(data_path, "annotations", "val", "tissue", num + ".png")
val_tissue_ground_truth_cropped_path = os.path.join(data_path, "annotations", "val", "cropped_tissue", num + ".png")

tissue_ground_truth = torch.from_numpy(cv2.imread(val_tissue_ground_truth_path)).permute(2, 0, 1)
tissue_ground_truth[tissue_ground_truth == 255] = 3
tissue_ground_truth -= 1
tissue_ground_truth = tissue_ground_truth[0].to(torch.long)


tissue_ground_truth_cropped = cv2.imread(val_tissue_ground_truth_cropped_path)
tissue_ground_truth_cropped = torch.from_numpy(cv2.cvtColor(tissue_ground_truth_cropped, cv2.COLOR_BGR2RGB))

tissue_ground_truth = one_hot(tissue_ground_truth, num_classes=3)
cell_image = image[:3]
tissue_pred_cropped = image[3:]
```

```{python}
print(cell_image.shape)
print(tissue_pred_cropped.shape)
print(label_cell.shape)
print(tissue_ground_truth.shape)
```

```{python}
converter = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])

tissue_image_input = cv2.imread(val_tissue_image_path).astype(np.float32)
tissue_image_input = cv2.cvtColor(tissue_image_input, cv2.COLOR_BGR2RGB)

tissue_image_input = torch.from_numpy(tissue_image_input).permute(2, 0, 1).to(device)
print(tissue_image_input.shape)
print(tissue_image_input.min())
print(tissue_image_input.max())
print(tissue_image_input.dtype)

outputs_tissue_model = tissue_model(tissue_image_input.unsqueeze(0)/255)
tissue_prediction = outputs_tissue_model[0].detach().cpu()
tissue_prediction = tissue_prediction.argmax(0)

tissue_prediction = converter[tissue_prediction]
print(tissue_prediction.shape)
```

```{python}
from skimage.feature import peak_local_max
from torch.nn.functional import softmax
from src.utils.utils import create_cell_segmentation_image

cell_predictions = tissue_cell_model(image.unsqueeze(0).to(device))
cell_prediction = cell_predictions[0].cpu().detach()

argmaxed_cell_prediction = cell_prediction.argmax(0)
argmaxed_cell_prediction = converter[argmaxed_cell_prediction.cpu().detach()]
# print(argmaxed_cell_prediction.shape)



## Peak local max

softmaxed = softmax(cell_prediction, dim=0)

# max values and indices
confidences, predictions = torch.max(softmaxed, dim=0)
confidences, predictions = confidences.numpy(), predictions.numpy()
peak_points_pred = peak_local_max(
    confidences,
    min_distance=20,
    labels=np.logical_or(predictions == 1, predictions == 2),
    threshold_abs=0.01,
)

n = peak_points_pred.shape[0]
annotations = np.zeros(shape=(n, 3))
annotations[:, 0] = peak_points_pred[:, 1]
annotations[:, 1] = peak_points_pred[:, 0]

annotations[:, 2] = predictions[peak_points_pred[:, 0], peak_points_pred[:, 1]]

for i in range(len(annotations[:, 2])):
    if annotations[i, 2] == 1: 
        annotations[i, 2] = 2
    elif annotations[i, 2] == 2: 
        annotations[i, 2] = 1

annotations = annotations.astype(np.int64)
peak_cell_prediction = create_cell_segmentation_image(annotations, cell_mpp=0.2)


# peak_cell_prediction = np.zeros(shape=(1024, 1024))
# peak_cell_prediction[peak_points_pred[:, 0], peak_points_pred[:, 1]] = predictions[peak_points_pred[:, 0], peak_points_pred[:, 1]]
# peak_cell_prediction = converter[peak_cell_prediction]
```

```{python}
print(num)

plt.figure(figsize=(20, 20))
plt.axis("off")

plt.subplot(3, 3, 1)
plt.imshow(cell_image.permute(1, 2, 0))
plt.title("Cell image")
plt.axis("off")

plt.subplot(3, 3, 2)
plt.imshow(tissue_pred_cropped.permute(1, 2, 0))
plt.title("Tissue image prediction cropped")
plt.axis("off")

plt.subplot(3, 3, 3)
plt.imshow(255*label_cell.permute(1, 2, 0))
plt.title("Cell Label")
plt.axis("off")

plt.subplot(3, 3, 4)
plt.imshow(255*argmaxed_cell_prediction)
plt.title("Argmaxed Model Predictions")
plt.axis("off")

plt.subplot(3, 3, 5)
plt.imshow(255*tissue_ground_truth)
plt.title("Tissue label")
plt.axis("off")

plt.subplot(3, 3, 6)
plt.imshow(cv2.imread(val_tissue_image_path))
plt.title("Tissue image")
plt.axis("off")

plt.subplot(3, 3, 7)
plt.imshow(255*tissue_prediction)
plt.title("Tissue prediction")
plt.axis("off")

plt.subplot(3, 3, 8)
plt.imshow(255*peak_cell_prediction)
plt.title("Peak cell prediction")
plt.axis("off")

plt.subplot(3, 3, 9)
plt.imshow(255*tissue_ground_truth_cropped)
plt.title("Tissue label cropped")
plt.axis("off")

plt.grid(color="white", linestyle="-", linewidth=2)
plt.tight_layout()
plt.show()
```
