---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: master_project
    language: python
    name: python3
---

```{python}
# Fixing automatic autoreload
# %load_ext autoreload
# %autoreload 2

import os 

# Making sure we are running the code from the root directory
current_directory = os.getcwd()
if current_directory.endswith("notebooks"):
    os.chdir("..")
    print("Changed directory to:", os.getcwd())
else:
    print("Directory was already correct, so did not change.")
```

```{python}
import cv2
import json
import torch
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F

from monai.transforms import SpatialCrop, Resize
from PIL import Image
from src.utils.utils import (
    read_data, 
    get_tissue_crops_scaled_tensor,
    get_partition_from_file_name
)

from src.deeplabv3.network.modeling import _segm_resnet
from src.utils.constants import IDUN_OCELOT_DATA_PATH

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"device: {device}")
```

```{python}
# Creating tissue model
backbone = "resnet50"
dropout_rate = 0.3
model_tissue = _segm_resnet(
    name="deeplabv3plus",
    backbone_name=backbone,
    num_classes=3,
    num_channels=3,
    output_stride=8,
    pretrained_backbone=True,
    dropout_rate=dropout_rate,
)
model_tissue.load_state_dict(
    torch.load(
        "outputs/models/2024-01-21_15-48-32_deeplabv3plus_tissue_branch_lr-1e-05_dropout-0.3_backbone-resnet50_epochs-30.pth"
    )
)
model_tissue.to(device)
model_tissue.eval()
print()
```

```{python}
# Find the paths for the tissue images
partition = "train"
```

```{python}
from PIL import Image
from torchvision.transforms.v2.functional import resized_crop
from torchvision.transforms import InterpolationMode


def validate_numpy_image(image: np.ndarray) -> None:
    """
    Validates that the image that was read from file is in the correct format.
    """
    if image.shape != (1024, 1024, 3):
        raise ValueError(f"Image shape is not (1024, 1024, 3), but {image.shape}")
    if image.dtype != np.uint8:
        raise ValueError(f"Image dtype is not np.uint8, but {image.dtype}")
    if image.min() < 0 or image.max() > 255:
        raise ValueError(
            f"Image values are not in the range [0, 255], but {image.min()} - {image.max()}"
        )
    if image.max() <= 1:
        raise ValueError(f"Image values are not in the range [0, 255], but [0, 1]")


def validate_torch_image(image_torch: torch.Tensor) -> None:
    """
    Validates that the image that was read from file is in the correct format.
    """
    if image_torch.shape != (3, 1024, 1024):
        raise ValueError(f"Image shape is not (3, 1024, 1024), but {image_torch.shape}")
    if image_torch.dtype != torch.float32:
        raise ValueError(f"Image dtype is not torch.float32, but {image_torch.dtype}")
    if image_torch.min() < 0 or image_torch.max() > 1:
        raise ValueError(
            f"Image values are not in the range [0, 1], but {image_torch.min()} - {image_torch.max()}"
        )


def get_torch_image(path: str) -> torch.Tensor:
    """
    Takes in a string as a path and returns a torch tensor of the image with
    shape (3, 1024, 1024) and dtype torch.float32, with values in [0, 1].
    """
    # Format: (1024, 1024, 3), 0-255, np.uint8
    image: np.ndarray = cv2.imread(path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    validate_numpy_image(image)

    # Fixing the format of the image
    image = image.astype(np.float32) / 255.0
    image_torch = torch.from_numpy(image).permute(2, 0, 1)
    validate_torch_image(image_torch)

    return image_torch


def crop_and_resize_tissue_prediction(
    image: torch.Tensor,
    tissue_mpp: float,
    cell_mpp: float,
    x_offset: float,
    y_offset: float,
    input_height: int = 1024,
    input_width: int = 1024,
) -> torch.Tensor:
    """
    Takes in an input image of a tissue prediction and crops and resizes it, 
    based on the given MPPs and offsets.

    Args:
        image (torch.Tensor): A 2D tensor of shape (input_height, input_width)
            representing the input image to be cropped and resized.
        tissue_mpp (float): The microscopy pixels per unit for the tissue image.
        cell_mpp (float): The microscopy pixels per unit for the cell image.
        x_offset (float): The horizontal offset (as a fraction of width) for the center
            of the crop area, must be between 0 and 1 inclusive.
        y_offset (float): The vertical offset (as a fraction of height) for the center
            of the crop area, must be between 0 and 1 inclusive.
        input_height (int, optional): The height of the input image. Defaults to 1024.
        input_width (int, optional): The width of the input image. Defaults to 1024.

    Returns:
        torch.Tensor: A tensor of the same shape as the input (input_height, input_width)
            containing the cropped and resized image.

    Raises:
        ValueError: If the input image does not have the expected shape.
        ValueError: If tissue_mpp is less than cell_mpp.
        ValueError: If either offset is not within the [0, 1] range.
        ValueError: If the calculated crop area extends beyond the bounds of the input image.

    """
    if image.shape != (input_height, input_width):
        raise ValueError(
            f"Image shape is not ({input_height}, {input_width}), but {image.shape}"
        )

    if tissue_mpp < cell_mpp:
        raise ValueError(f"Tissue mpp is less than cell mpp: {tissue_mpp} < {cell_mpp}")

    if not (0 <= x_offset <= 1) or not (0 <= y_offset <= 1):
        raise ValueError(f"Offsets are not in the range [0, 1]: {x_offset}, {y_offset}")

    # Calculating crop size and position
    scaling_value = cell_mpp / tissue_mpp
    assert 0 <= scaling_value <= 1

    crop_height = int(input_height * scaling_value)
    crop_width = int(input_width * scaling_value)

    # Note that the offset is the center of the cropped image
    top = int(y_offset * input_height - crop_height / 2)
    left = int(x_offset * input_width - crop_width / 2)

    if top < 0 or top + crop_height > input_height:
        raise ValueError(
            f"Top + crop height is not in the range [0, {input_height}]: {top}"
        )
    if left < 0 or left + crop_width > input_width:
        raise ValueError(
            f"Left + crop width is not in the range [0, {input_width}]: {left}"
        )

    image = image.unsqueeze(0)
    crop = resized_crop(
        inpt=image,
        top=top,
        left=left,
        height=crop_height,
        width=crop_width,
        size=(input_height, input_width),
        interpolation=InterpolationMode.NEAREST,
    )
    crop = crop.squeeze(0)

    return crop


def create_cropped_tissue_predictions(
    model_tissue: torch.nn.Module,
    partition: str = "train",
    device: torch.device = torch.device("cuda"),
    ocelot_data_path: str = IDUN_OCELOT_DATA_PATH,
) -> None:
    """
    Reads tissue images and processes them using a tissue prediction model. 
    Saves the results to file. 

    Args:
        model_tissue (torch.nn.Module): The deep learning model to apply to each tissue image.
        partition (str): The dataset partition to process, e.g., "train", "test", or "validate".
        device (torch.device): The computing device (CUDA or CPU) on which the model will run.
        ocelot_data_path (str): The base path to the dataset and metadata. 
    """

    # Getting the correct paths
    tissue_crop_path = os.path.join(
        ocelot_data_path, "annotations", partition, "cropped_tissue"
    )
    tissue_path = os.path.join(ocelot_data_path, "images", partition, "tissue")
    tissue_files = sorted(
        [
            os.path.join(tissue_path, path)
            for path in os.listdir(tissue_path)
            if ".jpg" in path
        ]
    )

    # Reading the metadata
    metadata_path = os.path.join(ocelot_data_path, "metadata.json")
    with open(metadata_path, "r") as file:
        metadata = json.load(file)

    # For each image, create a tissue crop and save the cropped version with the same name
    for path in tissue_files:
        image_num = path.split("/")[-1].split(".")[0]

        # Getting the relevant metadata for the sample
        sample_metadata = metadata["sample_pairs"][image_num]
        tissue_mpp = sample_metadata["tissue"]["resized_mpp_x"]
        cell_mpp = sample_metadata["cell"]["resized_mpp_x"]
        x_offset = sample_metadata["patch_x_offset"]
        y_offset = sample_metadata["patch_y_offset"]

        # Format: (3, 1024, 1024), 0-1, torch.float32
        image_torch = get_torch_image(path)

        # Feed the image into the model
        image_torch = image_torch.unsqueeze(0).to(device)
        model_tissue.to(device)
        with torch.no_grad():
            output = model_tissue(image_torch).squeeze(0)

        argmaxed = output.argmax(dim=0)

        # Crop the image to desired size
        cropped_image = crop_and_resize_tissue_prediction(
            argmaxed, tissue_mpp, cell_mpp, x_offset, y_offset
        )

        # NOTE: Permute puts the channels last, which is fine for cv2.imwrite
        one_hot = F.one_hot(cropped_image, num_classes=3)
        assert one_hot.sum(dim=2).unique().item() == 1

        one_hot = one_hot.cpu().numpy().astype(np.uint8)

        # Save the cropped image to file
        cv2.imwrite(
            filename=os.path.join(tissue_crop_path, f"{image_num}.png"),
            img=cv2.cvtColor(one_hot, cv2.COLOR_RGB2BGR)
        )


print("Creating images for train set")
create_cropped_tissue_predictions(model_tissue=model_tissue, partition="train")
print("Creating images for val set")
create_cropped_tissue_predictions(model_tissue=model_tissue, partition="val")
print("Creating images for test set")
create_cropped_tissue_predictions(model_tissue=model_tissue, partition="test")
```

```{python}
data_path = IDUN_OCELOT_DATA_PATH
  
train_data, val_data, test_data = read_data(data_path)
total_data = {}
total_data.update(train_data)
total_data.update(val_data)
total_data.update(test_data)
```

```{python}
# train_data = {k: train_data[k] for k in sorted(train_data, key=int)}
# val_data = {k: val_data[k] for k in sorted(val_data, key=int)}
# test_data = {k: test_data[k] for k in sorted(test_data, key=int)}

total_data = {k: total_data[k] for k in sorted(total_data, key=int)}
```

## Load tissue model

```{python}
backbone = "resnet50"
dropout_rate = 0.3
model_tissue = _segm_resnet(
    name="deeplabv3plus",
    backbone_name=backbone,
    num_classes=3,
    num_channels=3,
    output_stride=8,
    pretrained_backbone=True,
    dropout_rate=dropout_rate
)
model_tissue.to(device)
```

```{python}
# OBS: The comment below is a tissue model with wrong name
# outputs/models/2023-12-07_22-49-54_deeplabv3plus_cell_only_lr-0.0001_dropout-0.3_backbone-resnet50_epochs-290.pth
model_tissue.load_state_dict(torch.load("outputs/models/2024-01-21_15-48-32_deeplabv3plus_tissue_branch_lr-1e-05_dropout-0.3_backbone-resnet50_epochs-30.pth"))
```

```{python}
## Changing the
model_tissue.eval()
for key in total_data.keys():
    tissue_image = total_data[key]["tissue_image"]
    tissue_image = tissue_image.to(device).to(torch.float).unsqueeze(0)

    prediction = model_tissue(tissue_image / 255).squeeze(0)

    prediction = torch.argmax(prediction, 0)
    convert = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]).to(device)
    prediction = convert[prediction].permute(2, 0, 1)

    # plt.imshow(prediction)
    # plt.show()

    total_data[key]["tissue_annotated"] = prediction
```

```{python}
train_folder = os.path.join(data_path, "annotations/train/pred_tissue")
val_folder = os.path.join(data_path, "annotations/val/pred_tissue")
test_folder = os.path.join(data_path, "annotations/test/pred_tissue")
os.makedirs(train_folder, exist_ok=True)
os.makedirs(val_folder, exist_ok=True)
os.makedirs(test_folder, exist_ok=True)
```

```{python}
for (key, value) in total_data.items(): 
    temp_dict = {key: value}
    temp_dict[key]["tissue_annotated"] = value["tissue_annotated"].to("cpu")
    cropped_tensor = get_tissue_crops_scaled_tensor(data=temp_dict)
    temp_dict[key]["tissue_annotated"] = value["tissue_annotated"].to(device)
    
    cropped_tensor = cropped_tensor[:, 3:]

    partition = get_partition_from_file_name(key)
    image_folder = os.path.join(data_path, "annotations", partition, "pred_tissue")

    img = Image.fromarray(cropped_tensor.squeeze().to(torch.uint8).permute(1, 2, 0).numpy())
    img.save(f"{image_folder}/{key}.png")
```
